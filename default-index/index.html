<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo_treeby.svg" color="#222">
  <link rel="manifest" href="/images/manifest.json">
  <meta name="msapplication-config" content="/images/browserconfig.xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs | buttons","Available values":"disqus | changyan | disqus | disqusjs | gitalk | livere | valine","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="La vie est belle">
<meta property="og:type" content="website">
<meta property="og:title" content="Amusement Park">
<meta property="og:url" content="http://example.com/default-index/index.html">
<meta property="og:site_name" content="Amusement Park">
<meta property="og:description" content="La vie est belle">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Treeby">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/default-index/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Amusement Park</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Amusement Park</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Treeby's blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-github">

    <a href="https://github.com/satreeby" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>Github</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/11/15/Kernel_Computation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/11/15/Kernel_Computation/" class="post-title-link" itemprop="url">Kernel_Computation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-11-15 19:44:54 / Modified: 19:47:27" itemprop="dateCreated datePublished" datetime="2023-11-15T19:44:54+08:00">2023-11-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Notes-of-E%EF%AC%83cient-Processing-of-Deep-Neural-Networks/" itemprop="url" rel="index"><span itemprop="name">Notes of Eﬃcient Processing of Deep Neural Networks</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>the fundamental computation are <strong>multiply-and-accumulate(MAC)</strong> operations</li>
<li>Temporal architecture(CPU, GPU)</li>
<li>Spatial architecture(ASIC, FPGA)</li>
</ul>
<p><img src="/2023/11/15/Kernel_Computation/1700046066156.png" alt="1700046066156"></p>
<h2 id="Temporal-Arichitecure"><a href="#Temporal-Arichitecure" class="headerlink" title="Temporal Arichitecure"></a>Temporal Arichitecure</h2><p>DNN computations can be formulated as a matrix multiplication</p>
<ul>
<li><strong>Toeplitz Matrix</strong></li>
</ul>
<p>Example(1):</p>
<blockquote>
<p>a FC(full connect) layer:<br><strong>$O_{nm}&#x3D;\sum_{chw} I_{nchw}F_{mchw}$</strong><br><img src="/2023/11/15/Kernel_Computation/1700046522900.png" alt="1700046522900"><br>Turning to the matrix multilplication, means turning one filter into a row vector, one input into a col vector<br><img src="/2023/11/15/Kernel_Computation/1700046639970.png" alt="1700046639970"></p>
</blockquote>
<p>Example(2):</p>
<blockquote>
<p>a CONV layer:<br><strong>$\begin{aligned}<br>&amp;O[n][m][p][q]&#x3D;((\sum_{c&#x3D;0}^{C-1} \sum_{r&#x3D;0}^{R-1} \sum_{s&#x3D;0}^{S-1}i[n][c][Uq+s]\times f[m][c][r][s])+b[m]).\\<br>&amp;0\leq n\lt N, 0\leq m\lt M, 0\leq p\lt P, 0\leq q\lt Q,\\<br>&amp;P&#x3D;(H-R+U)&#x2F;U, Q&#x3D;(Q-S+U)&#x2F;U.<br>\end{aligned}$</strong><br><img src="/2023/11/15/Kernel_Computation/1700046905051.png" alt="1700046905051"><br>Turning to the matrix multilplication, for example:<br>a 2-D convolution:<br><img src="/2023/11/15/Kernel_Computation/1700047588149.png" alt="1700047588149"><br>higher dimention:<br><img src="/2023/11/15/Kernel_Computation/1700047616484.png" alt="1700047616484"><br>generally speaking:<br><img src="/2023/11/15/Kernel_Computation/1700047791426.png" alt="1700047791426"></p>
</blockquote>
<ul>
<li><strong>Tiling Algorithm</strong></li>
</ul>
<p><img src="/2023/11/15/Kernel_Computation/1700048102232.png" alt="1700048102232"></p>
<h2 id="Computation-Transform"><a href="#Computation-Transform" class="headerlink" title="Computation Transform"></a>Computation Transform</h2><p>Objection: to reduce the number of <em><strong>multiplications</strong></em> by increasing the number of <em><strong>additions</strong></em></p>
<ul>
<li>Strassen’s algorithm</li>
<li>Winogard’s algorithm</li>
<li>Fast Fourier Transform</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/11/15/Designing_DNN_Accelerators/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/11/15/Designing_DNN_Accelerators/" class="post-title-link" itemprop="url">Designing_DNN_Accelerators</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-11-15 19:44:54" itemprop="dateCreated datePublished" datetime="2023-11-15T19:44:54+08:00">2023-11-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-11-17 00:35:49" itemprop="dateModified" datetime="2023-11-17T00:35:49+08:00">2023-11-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Notes-of-E%EF%AC%83cient-Processing-of-Deep-Neural-Networks/" itemprop="url" rel="index"><span itemprop="name">Notes of Eﬃcient Processing of Deep Neural Networks</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="key-metrics">Key Metrics</h2>
<ul>
<li><strong>Energy consumption</strong>
<ol type="1">
<li>减少数据的移动(data movement)</li>
<li>减少移动单位数据的代价</li>
</ol></li>
<li><strong>Performance in terms of throughput and latency</strong>
<ol type="1">
<li>增加并行度(parallel)</li>
<li>减少PE空闲周期数</li>
</ol></li>
<li><strong>Area</strong></li>
</ul>
<h2 id="key-properties-of-dnn-to-leverage">Key Properties of DNN to
Leverage</h2>
<p><strong><em>Data reuse</em></strong></p>
<ul>
<li><strong>Input feature map reuse</strong>
不同的滤波器(M)，相同的输入</li>
<li><strong>Filter reuse</strong> 同一个滤波器，不同样本(N)的输入</li>
<li><strong>Convolutional reuse</strong>
<ol type="1">
<li>每个filter的每个weight，复用 <span class="math inline">\(P\times
Q\)</span> 次，产生 <span class="math inline">\(P\times Q\)</span> 个
output 像素点</li>
<li>若步长 U=1，则每个 input 的像素，一般会与 <span class="math inline">\(R\times S\)</span> 个filter weight相乘，得到的
psum 属于不同的 output 像素</li>
</ol></li>
<li><strong>Movement of Partial Sums</strong> 得到一个输出的像素点，有
<span class="math inline">\(C \times R \times S\)</span> 个 psum 需要被
accumulated</li>
</ul>
<h2 id="dnn-hardware-design-consideratuions">DNN hardware design
Consideratuions</h2>
<p>设计一个 DNN 加速器，大致需要以下步骤：</p>
<ul>
<li><strong>At design time</strong>
<ol type="1">
<li>dataflow(s)</li>
<li>PE(process element)的数量，以及每个PE执行的MAC数量</li>
<li>the memory
hierarchy，包括存储的级数(levels)，以及每一级的容量(capicity)</li>
<li>the allowed pattern of NoC(Network on-chip)</li>
</ol></li>
<li><strong>At mapping time</strong> 将 DNN
网络有效的映射到硬件加速器上</li>
<li><strong>At configuration time</strong></li>
<li><strong>At run time</strong></li>
</ul>
<h2 id="data-reuse">Data reuse</h2>
<ul>
<li><strong>Temporal reuse</strong></li>
</ul>
<blockquote>
<ul>
<li><strong><em>Temporal reuse occurs when the same data value is used
more than once by the same consumer</em></strong></li>
<li>It can be exploited by adding an intermediate memory level to the
memory hierarchy of the hardware, where the intermediate memory level
has a smaller storage capacity than the level that acts as the original
source of the data</li>
<li>For exploiting temporal reuse, the reuse distance is defined as the
number of data accesses required by the consumer in between the accesses
to the same data value, which is a function of the ordering of
operations.</li>
</ul>
</blockquote>
<p>比如，以下1-D卷积，filter weight
的存储有两级，L0存放着所有权重，L1仅能存放数量为一的权重，如(d)，则两种不同的计算顺序(order)，其
weight reuse distance 也不同，(output reuse distance 也不同)。 <img src="/2023/11/15/Designing_DNN_Accelerators/1700057619410.png" alt="任我行"></p>
<p>因此，重要的是，<strong>reuse distance, 即 the processing
order</strong></p>
<ul>
<li><strong>Spatial reuse</strong></li>
</ul>
<blockquote>
<ul>
<li><strong><em>Spatial reuse occurs when the same data value is used by
more than one consumer (e.g., a group ofPEs) at different spatial
locations ofthe hardware.</em></strong></li>
<li>It can be exploited by reading the data once fromthe source memory
level andmulticasting it to all ofthe consumers.</li>
<li>For exploiting spatial reuse, the reuse distance is defined as the
maximum number of data accesses in between any pair ofconsumers that
access the same data value, which is again a function of the ordering of
operations.</li>
</ul>
</blockquote>
<p>比如，以下1-D卷积，有数量为4的 consumers(PE)，可用于同时计算 MAC，
<img src="/2023/11/15/Designing_DNN_Accelerators/1700058040471.png" alt="任我行"></p>
<p>因此，重要的是，<strong>reuse distance, 即 the designs of
parallel</strong></p>
<p>同时需要注意，spatial reuse 需要 Noc 来支持，与 Memory 的层级和带宽
(bandwidth) 都有关系。</p>
<ul>
<li>data reuse 设计关键点
<ol type="1">
<li>Process order</li>
<li>Parallel computation</li>
<li>Data tiling</li>
<li>同时考虑 bandwidth, Memory hierarchy 等</li>
</ol></li>
<li>算法(Algorithm)描述</li>
</ul>
<blockquote>
<ul>
<li>loop nests</li>
<li>WS, IS, OS</li>
<li><strong><em>for</em></strong></li>
</ul>
</blockquote>
<p>比如，使用伪代码描述下面一个1-D卷积，循环越靠近外层，说明其静态性(stationary)越高，
<img src="/2023/11/15/Designing_DNN_Accelerators/1700058946892.png" alt="任我行">
<img src="/2023/11/15/Designing_DNN_Accelerators/1700059062834.png" alt="任我行">
以图 (a) 的 WS 为例，完成本次卷积所有操作的 0-35 次 cycle 里面，追踪每次
cycle 用到的数据的索引 (Index) 如下图： <img src="/2023/11/15/Designing_DNN_Accelerators/1700059237627.png" alt="任我行"></p>
<blockquote>
<p><strong><em>parallel-for</em></strong></p>
</blockquote>
<p>比如，对上述的 <strong>WS</strong> 加入对 <strong>权重</strong>
的并行设计，即将权重的循环更进行的划分(Tiling)： <img src="/2023/11/15/Designing_DNN_Accelerators/1700059779411.png" alt="任我行">
每次执行的状况如下： <img src="/2023/11/15/Designing_DNN_Accelerators/1700059801549.png" alt="任我行">
可将之与无并行进行对比。 设计并行时，既要考虑 PE 数量，又要考虑 Memory
的容量与带宽，还要考虑划分是否整除，等等。</p>
<blockquote>
<p>A dataflow only defines the following aspects of a loop nest: (1) the
specific order of the loops to prioritize the data types; (2) the number
of loops for each data dimension to describe the tiling; and (3) whether
each of the loops is temporal (for) or spatial (parallel-for).</p>
</blockquote>
<h2 id="dataflow-taxonomy">Dataflow Taxonomy</h2>
<h3 id="weight-stationaryws">Weight Stationary(WS)</h3>
<blockquote>
<ul>
<li>The weight-stationary dataflow is designed to minimize the energy
consumption of reading weights by maximizing the reuse ofweights from
the register file (RF) at each PE</li>
<li>The processing runs as manyMACs that use the same weight as possible
while the weight is present in the RF</li>
<li>The inputs and partial sums must move through the spatial array and
global buffer. The input feature map activations are broadcast to all
PEs and then the partial sums are spatially accumulated across the PE
array.</li>
</ul>
</blockquote>
<figure>
<img src="/2023/11/15/Designing_DNN_Accelerators/1700062976339.png" alt="任我行">
<figcaption aria-hidden="true">任我行</figcaption>
</figure>
<p>比如，nn-X (also called neuFlow)，它有：</p>
<ul>
<li>8 个 2-D CONV engines</li>
<li>每个 engine 有 100 个 PE（因此可以实现最多<span class="math inline">\(10 \times 10\)</span>的卷积）</li>
<li>weight 在每个 PE 里保持静态不变</li>
<li>input 广播(broadcast) 至每个 PE 中</li>
<li>由于每次(cycle)所广播的只有一个 input
像素，为保证时序正确性，需要添加寄存器以控制 psum 的相加</li>
</ul>
<figure>
<img src="/2023/11/15/Designing_DNN_Accelerators/1700063409653.png" alt="任我行">
<figcaption aria-hidden="true">任我行</figcaption>
</figure>
<p>再比如，Nvidia’s Deep Learning Accelerator
(NVDLA)，与前面不同的是，psum 是一列列相加（不同列不同的 filter
，每一行则对应不同的 input(and filter) channel <img src="/2023/11/15/Designing_DNN_Accelerators/1700063949353.png" alt="任我行">
其算法如下： <img src="/2023/11/15/Designing_DNN_Accelerators/1700064715944.png" alt="任我行"></p>
<h3 id="output-stationaryos">Output Stationary(OS)</h3>
<blockquote>
<ul>
<li>The output-stationary dataflow is designed to minimize the energy
consumption of reading and writing the partial sums.</li>
<li>In order to keep the accumulation of partial sums stationary in the
RF, one common implementation is to stream the input activations across
the PE array and broadcast the weights to all PEs in the array from the
global buffer.</li>
</ul>
</blockquote>
<figure>
<img src="/2023/11/15/Designing_DNN_Accelerators/1700065902277.png" alt="任我行">
<figcaption aria-hidden="true">任我行</figcaption>
</figure>
<p>可以选择不同的 output 像素保持 stationary <img src="/2023/11/15/Designing_DNN_Accelerators/1700065547456.png" alt="任我行"></p>
<p>未完待续。。。</p>
<h3 id="input-stationaryis">Input Stationary(IS)</h3>
<blockquote>
<ul>
<li>Similar to the previous two dataflows, the input-stationary dataflow
is designed to minimize the energy consumption ofreading input
activations.</li>
<li>With minimized reuse distance, each input activation is read from
DRAM and put into the RF ofeach PE and stays stationary for further
access. Then, it runs through as many MACs as possible in the PE to
reuse the same input activation.</li>
<li>While each input activation stays stationary in the RF, unique
filter weights are uni-cast into the PEs at each cycle, while the
partial sums are spatially accumulated across the PEs to generate the
final output activation.</li>
</ul>
</blockquote>
<figure>
<img src="/2023/11/15/Designing_DNN_Accelerators/1700124708788.png" alt="任我行">
<figcaption aria-hidden="true">任我行</figcaption>
</figure>
<p>未完待续。。。</p>
<h3 id="row-stationaryrs">Row Stationary(RS)</h3>
<p>A row-stationary dataflow is proposed in <em>Eyeriss</em>, which aims
to maximize the reuse and accumulation at the RF level for all types of
data (weights, input activations, and partial sums) for the overall
energy efficiency.</p>
<ul>
<li><strong>1-D CONV</strong>：</li>
</ul>
<blockquote>
<p>weight stationary inside the RF streams the activations into PE
比如下面 S=3，W=5，U=1 的卷积，得到 F=3 的 output，需要三个 step <img src="/2023/11/15/Designing_DNN_Accelerators/1700134333058.png" alt="任我行"><img src="/2023/11/15/Designing_DNN_Accelerators/1700134346761.png" alt="1700134346761"><img src="/2023/11/15/Designing_DNN_Accelerators/1700134358771.png" alt="1700134358771"> 值得注意的是，filter 与 input 流进 PE 与 output
流出，都应使用 FIFO 进行正确的时序控制</p>
</blockquote>
<ul>
<li><strong>2-D CONV</strong>：</li>
</ul>
<blockquote>
<p>由 多个 1-D 卷积组成， 比如下面 R=3, S=3, H=5, W=5, U=1 的卷积，得到
E=3, F=3 的 output <img src="/2023/11/15/Designing_DNN_Accelerators/1700135701893.png" alt="任我行">
****</p>
</blockquote>
<blockquote>
<ol type="1">
<li>在这个例子中，每一行 output 的产生，仍然需要 3 个 steps</li>
<li>而每一个 step 所产生的一个 output 像素点，都需要 3 row 的 1-D
卷积同时进行，然后将对应的 psum 相加(accumulated) 得到</li>
<li>得到所有行的 output 需要进行 3 次上述的操作</li>
<li><strong>由上述例子可得：每一 Row 的 filter weights 在同一 Row 的 PE
上复用；每一 ROW 的 inputs 则按 相同的 diagnal 复用； 每一 col 的 PE
得到的 psum 要进行相加</strong></li>
</ol>
</blockquote>
<ul>
<li><strong>Higher-dimensional CONV</strong>:</li>
</ul>
<blockquote>
<p><strong>N, M, C</strong>三个维度 <strong>Interleaving</strong> or
<strong>concatenating</strong> 有下面三种复用：(a) filter reuse of
multiple (<strong>N</strong>) input (b) input reuse of multiple
(<strong>M</strong>) filters (c) output reuse of multiple filter weights
and inputs (<strong>C</strong>) <img src="/2023/11/15/Designing_DNN_Accelerators/1700136845875.png" alt="任我行">
对应的techniques可以是：(a) concatenating (b) interleaving (c)
interleaving <img src="/2023/11/15/Designing_DNN_Accelerators/1700136924461.png" alt="任我行"> 存在问题：PE 阵列的大小是 fixed 的，应如何将不同 shape
的 DNN layer 映射呢？两大techniques：(a) replication (b) folding <img src="/2023/11/15/Designing_DNN_Accelerators/1700137378744.png" alt="任我行">
总的思想是：提高 PE 的利用率 注意：不使用的 PE 应关上使其耗能减少。</p>
</blockquote>
<ul>
<li><strong>硬件结构支持</strong></li>
</ul>
<ol type="1">
<li>PE array</li>
<li>Mapping and Configure</li>
<li>Noc</li>
<li>Memory hierarchy</li>
</ol>
<h3 id="other-stationary">Other Stationary</h3>
<blockquote>
<ul>
<li>Since the stationariness of a dataflow is only relative to a
specific level of memory hierarchy, different stationariness can be used
at each level of the memory hierarchy.</li>
<li>From the perspective ofa loop nest, it involves tiling the various
data types into multiple levels and then reordering the loops at
different levels to prioritize different data types.</li>
<li>For example, ifthere are K levels ofmemory hierarchy, both dimension
Q and S in the loop nest ofFigure 5.8a can be further divided up into K
loops (i.e., for the open ranges [Q0, QK) and [S0 to SK) ) and then
reordering the loops independently from loop level 0 to level K -
1.</li>
<li><strong>This also implies that smaller DNN accelerator designs can
be further combined together with a new level ofmemory to form a larger
accelerator.</strong></li>
<li><strong>The key to these designs is to propose flexible NoCs and
support various memory access patterns in order to execute different
dataflows that have different numbers ofloop levels and loop ordering in
the loop nest</strong></li>
<li>上面的讨论基于一个layer无法在PE中一次就计算完成，然而，若layer足够小，或PE足够大，还需讨论<strong>跨层处理、流水线</strong>等技术</li>
</ul>
</blockquote>
<h2 id="dnn-accelerator-buffer-management-strategies">DNN Accelerator
Buffer Management Strategies</h2>
<p>可大致分为四种结构：</p>
<ul>
<li>implicit/explicit（隐式/显式）:
指的是利用工作负载知识来控制数据缓冲决策的程度</li>
<li>coupled/decoupled（耦合/解耦）:
指的是内存响应和请求是往返的(请求-响应)还是前向的(数据自动推送给使用者)
<img src="/2023/11/15/Designing_DNN_Accelerators/1700150870447.png" alt="1700150870447"></li>
</ul>
<blockquote>
<ul>
<li>In the general-purpose computing community, <strong>caches</strong>
are the predominant buffering mechanism and are based on load/store
(i.e., round-trip) operations</li>
<li>Caches can be characterized as performing implicit data
orchestration as the load request initiator does not directly control
the cache hierarchy’s decisions about whether the response data is
retained at any given level of the storage hierarchy, nor when it is
removed</li>
<li>for DNN accelerators, the area and energy overheads for features
like tag matches and associative sets are high, and so far as we are
aware no contemporary DNN accelerator incorporate caches</li>
<li>未完待续。。。</li>
</ul>
</blockquote>
<hr>
<blockquote>
<ul>
<li>An alternative to caches is to use <strong>scratchpads</strong>,
which expose an address range ofa particular staging buffer for loads
and stores, thereby enabling explicit and precise control over the data
orchestration.</li>
<li>A GPU’s shared memory scratchpad [171] is a widespread contemporary
example ofthis idiomfor explicit data orchestration. The size and
address range ofthe scratchpad is exposed architecturally, and the
transfer of data into and out of the scratchpad is managed via explicit
instructions.</li>
<li>While scratchpads avoid the hardware overheads of caches, extracting
memory parallelism—both across fills and overlapping fills and
accesses—is tedious and errorprone, and as a result they are difficult
to compose into hierarchie</li>
<li>未完待续。。。</li>
</ul>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/11/15/2023%E7%A7%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/11/15/2023%E7%A7%8B/" class="post-title-link" itemprop="url">2023秋</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-11-15 00:45:54 / Modified: 00:55:27" itemprop="dateCreated datePublished" datetime="2023-11-15T00:45:54+08:00">2023-11-15</time>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="2023-11-15"><a href="#2023-11-15" class="headerlink" title="2023.11.15"></a>2023.11.15</h2><p>开学十一个星期了，感觉自己过得一塌糊涂。<br>本以为，是以三天数模奋战开始的斗志昂扬的一个学期，结果省三的屑成绩再次证明了自己真的是屑。我是多么期盼，果酱名单上有我们队伍的名字，并写着三个成员均来自二中，但那终究只存在于梦里。</p>
<p>科研，也是一样。虽说进了所谓的实验室，但是，0项目，0论文，我也不知道在做些啥，现在还在看eyeriss论文和Eﬃcient Processing of Deep Neural Networks一书。</p>
<p>绩点，上学期3.78&#x2F;4.0，这学期感觉还会往下掉，半器、dsp全靠看网课自学，数集抓不住重点，感觉都是重点，也不知道是不是重点，数据结构似乎学的还可以。通识课被卷死了，经济学原理还没怎么看书。</p>
<p>绩点、竞赛、科研，唉，真的一塌糊涂。每次我这样想，我也经常这样想，压力、焦虑……我只是想，明年这个时候，有书读而已。常常想，微电子科学与工程，这个专业，究竟选对了没？虽然，这个问题，可能没有答案。但是，这学期来，我已经因为这些，几次感觉到压抑并且喘不过气来。我不知道会不会是抑郁，但如果没有二中的朋友和音乐，我感觉我真的没法活。</p>
<p><img src="/2023/11/15/2023%E7%A7%8B/1699979698443.png" alt="焦焦焦"></p>
<p>还有这段时间，又为感情的琐事纠缠了很久，当然，缘由我起，现在黯然落幕了，也说不了什么。</p>
<p><img src="/2023/11/15/2023%E7%A7%8B/1699979406940.png" alt="true friends or tree hole？"></p>
<blockquote>
<p>即使一生多出一根刺<br>没有刺痛别要知<br>就当共你有剧情没有故事</p>
</blockquote>
<p><em><strong>像你没来过没去过~</strong></em></p>
<p>迷茫的树安，现在只能选择继续相信，彼方尚有荣光在。这学期还剩不到两个月，得振作起来了，学业最重要！！！</p>
<p><strong>提前祝我亲爱的二中，93周年生日快乐！</strong></p>
<p><img src="/2023/11/15/2023%E7%A7%8B/1699980268904.png" alt="二中！我的二中！"></p>
<p><strong>今晚梦里，必归去兮！</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-10-31 00:32:29" itemprop="dateCreated datePublished" datetime="2023-10-31T00:32:29+08:00">2023-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-11-14 23:58:15" itemprop="dateModified" datetime="2023-11-14T23:58:15+08:00">2023-11-14</time>
              </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Eyeriss-An-Energy-Efficient-Reconfigurable-Accelerator-for-Deep-Convolutional-Neural-Networks"><a href="#Eyeriss-An-Energy-Efficient-Reconfigurable-Accelerator-for-Deep-Convolutional-Neural-Networks" class="headerlink" title="Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks"></a>Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h2 id="CNN-BASICS"><a href="#CNN-BASICS" class="headerlink" title="CNN BASICS"></a>CNN BASICS</h2><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698684647207.png" alt="1698684647207"></p>
<ul>
<li><em><strong>filters</strong></em></li>
<li><em><strong>infmaps</strong></em>(input feature maps)</li>
<li><em><strong>ofmaps</strong></em>(output feature maps)</li>
<li><em><strong>psums</strong></em>(partial sums)</li>
<li><em><strong>bias</strong></em></li>
</ul>
<p>$\begin{aligned}<br>O[z][u][x][y] &amp;&#x3D; ReLU(B[u]+\sum\limits_{k&#x3D;0}^{C-1} \sum\limits_{i&#x3D;0}^{R-1} \sum\limits_{j&#x3D;0}^{S-1} I[z][k][Ux+i][Uy+j]\times W[u][k][i][j])\\<br>&amp;0\leq z \lt N, 0\leq u \lt M, 0\leq y \lt E, 0\leq x \lt F,  \\<br>&amp;E&#x3D;(H-R+U)&#x2F;U, F&#x3D;(W-S+U)&#x2F;U  \\<br>\end{aligned}$</p>
<ul>
<li><em><strong>z</strong></em>指第z个样本</li>
<li><em><strong>u</strong></em>指该样本经过第u个filter得到第u个结果</li>
<li><em><strong>x,y</strong></em>指结果中第y行，第x列的像素</li>
<li><em><strong>k</strong></em>指第k个通道</li>
<li><em><strong>U</strong></em>代表步长</li>
<li><em><strong>filters</strong></em>维度为$M\times S\times R\times C$</li>
<li><em><strong>ifmaps</strong></em>维度为$(N\times) W\times H\times C$</li>
<li><em><strong>ofmaps</strong></em>维度为$(N\times) M\times F\times E$</li>
</ul>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698685541855.png" alt="1698685541855"></p>
<p>AlexNet共有五个卷积层，每个卷积层都包含卷积核、偏置项、ReLU激活函数和局部响应归一化（LRN）模块。</p>
<h3 id="卷积层C1"><a href="#卷积层C1" class="headerlink" title="卷积层C1"></a>卷积层C1</h3><ul>
<li><em><strong>filters</strong></em>: 96 × 11 × 11 × 3</li>
<li><em><strong>ifmaps</strong></em>：1 × 224 × 224 × 3</li>
<li><em><strong>stride</strong></em>: 4</li>
<li><em><strong>ofmaps</strong></em>: 55 × 55 × (48 × 2)</li>
<li>ReLU激活函数</li>
<li>最大池化后：2 × 27 × 27 × 48</li>
</ul>
<h3 id="卷积层C2"><a href="#卷积层C2" class="headerlink" title="卷积层C2"></a>卷积层C2</h3><ul>
<li><em><strong>filters</strong></em>: 256 × 5 × 5 × 48</li>
<li><em><strong>ifmaps</strong></em>：2 × (27+4) × (27+4) × 48</li>
<li><em><strong>stride</strong></em>: 1</li>
<li><em><strong>ofmaps</strong></em>: 27 × 27 × (128 × 2)</li>
<li>ReLU激活函数</li>
<li>最大池化后：2 × 13 × 13 × 128</li>
</ul>
<h3 id="卷积层C3"><a href="#卷积层C3" class="headerlink" title="卷积层C3"></a>卷积层C3</h3><ul>
<li><em><strong>filters</strong></em>: 384 × 3 × 3 × 256</li>
<li><em><strong>ifmaps</strong></em>：(13+2) × (13+2) × (128 × 2)</li>
<li><em><strong>stride</strong></em>: 1</li>
<li><em><strong>ofmaps</strong></em>: 13 × 13 × (192 × 2)</li>
<li>ReLU激活函数</li>
</ul>
<h3 id="卷积层C4"><a href="#卷积层C4" class="headerlink" title="卷积层C4"></a>卷积层C4</h3><ul>
<li><em><strong>filters</strong></em>: 384 × 3 × 3 × 192</li>
<li><em><strong>ifmaps</strong></em>：2 × (13+2) × (13+2) × 192</li>
<li><em><strong>stride</strong></em>: 1</li>
<li><em><strong>ofmaps</strong></em>: 13 × 13 × (192 × 2)</li>
<li>ReLU激活函数</li>
</ul>
<h3 id="卷积层C5"><a href="#卷积层C5" class="headerlink" title="卷积层C5"></a>卷积层C5</h3><ul>
<li><em><strong>filters</strong></em>: 256 × 3 × 3 × 192</li>
<li><em><strong>ifmaps</strong></em>：2 × (13+2) × (13+2) × 192</li>
<li><em><strong>stride</strong></em>: 1</li>
<li><em><strong>ofmaps</strong></em>: 13 × 13 × (128 × 2)</li>
<li>ReLU激活函数</li>
<li>最大池化层</li>
</ul>
<h2 id="SYSTEM-ARCHITECTURE"><a href="#SYSTEM-ARCHITECTURE" class="headerlink" title="SYSTEM ARCHITECTURE"></a>SYSTEM ARCHITECTURE</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698736236503.png" alt="1698736236503"></p>
<ul>
<li>two clock domain<ul>
<li>the core clock domain for processing</li>
<li>the link clock domain for communicating</li>
</ul>
</li>
<li>DRAM</li>
<li>a 64 bit bidirectional data bus</li>
<li>an asynchronous FIFO interface</li>
<li>a spatial array of 168 PEs<ul>
<li>12 × 16 rectangle</li>
<li>108-kB GLB</li>
<li>RLC CODEC</li>
</ul>
</li>
<li>an ReLU module</li>
<li>an NoC</li>
</ul>
<h3 id="System-Control-and-Configuration"><a href="#System-Control-and-Configuration" class="headerlink" title="System Control and Configuration"></a>System Control and Configuration</h3><ul>
<li><p>two levels of control hierarchy</p>
<ul>
<li>top level<ol>
<li>traffic between the off-chip DRAM and the GLB through the asynchronous interface</li>
<li>traffic between the GLB and the PE array through the NoC; and</li>
<li>operation of the RLC CODEC and ReLU module.</li>
</ol>
</li>
<li>lower level<br>control logic in each PE</li>
</ul>
</li>
<li><p>reconfigure first offline</p>
</li>
</ul>
<h2 id="ENERGY-EFFICIENT-FEATURES"><a href="#ENERGY-EFFICIENT-FEATURES" class="headerlink" title="ENERGY-EFFICIENT FEATURES"></a>ENERGY-EFFICIENT FEATURES</h2><h3 id="Energy-Efficient-Dataflow-Row-Stationary-RS"><a href="#Energy-Efficient-Dataflow-Row-Stationary-RS" class="headerlink" title="Energy-Efficient Dataflow: Row Stationary(RS)"></a>Energy-Efficient Dataflow: Row Stationary(RS)</h3><ol>
<li><strong>reducing data movement</strong></li>
<li><strong>exploiting data statistics</strong></li>
</ol>
<ul>
<li><p><em><strong>Data Reusing</strong></em></p>
<ol>
<li>Convolutional Reuse:<ul>
<li>Each filter weight is reused E × F times in the same ifmap plane</li>
<li>each ifmap pixel is usually reused R × S times in the same filter plane(when stride &#x3D; 1)</li>
</ul>
</li>
<li>Filter Reuse:<ul>
<li>Each filter weight is reused across the batch of N ifmaps</li>
</ul>
</li>
<li>Ifmaps Reuse:<ul>
<li>Each ifmap pixel is reused across M filters</li>
</ul>
</li>
</ol>
</li>
<li><p><em><strong>1-D Convolution Primitive in a PE</strong></em><br>每个 <strong>PE(Process Element)</strong> 先计算一维的卷积<br><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698753420423.png" alt="1698753420423"><br>filter, ifmap 皆从 spad 中取数据<br>psum 也存到 spad 中<br>因此，每个 PE 都需要配备一个</p>
<ol>
<li><strong>filter spad(capcity&#x3D;S)</strong></li>
<li><strong>ifmaps spad(capcity&#x3D;S)</strong></li>
<li><strong>psum spad(capcity&#x3D;1)</strong></li>
</ol>
</li>
<li><p><em><strong>2-D Convolution PE Set</strong></em><br>每个2D卷积由若干个1D卷积构成</p>
<ol>
<li>filter reuse<br> each row of filter is reused horizontally</li>
<li>ifmaps reuse<br> each row of ifmap is reused diagonally</li>
<li>final result of 2-D<br> rows of psum are accumulated vertically<br>以 filter&#x3D;3 × 3, ifmaps&#x3D;5 × 5, ofmaps&#x3D;3 × 3 (stride&#x3D;1) 为例，一个 PE Set 如下<br><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698754655474.png" alt="1698754655474"><br>因此，$PE_{i,j}$ 计算的是 ofmaps 中的 第 j 行，第 ？ 列的像素的 2-D 卷积中的第 i 行 1-D 卷积<br>因此，the height and the width of the PE set are equal to the number of filter rows (R) and ofmap rows (E)</li>
</ol>
<ul>
<li><em><strong>PE Set Mapping</strong></em><br>将 2-D Convolution PE Set(R × E) 映射到 PE 计算阵列 (12 × 14) 中。</li>
</ul>
<ol>
<li>R&lt;&#x3D;12 &amp; E&lt;&#x3D;14: 直接映射</li>
<li>R&lt;&#x3D;12 &amp; E&gt;14: 分批次映射，E&#x3D;e1+e2+e3+…</li>
<li>R&gt;12: 无解<br>以 AlexNet 为例，可以进行如下映射<br><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698756278238.png" alt="1698756278238"></li>
</ol>
</li>
<li><p><em><strong>Dimensions Beyond 2-D in PE Array</strong></em></p>
<ol>
<li>batch size N<br> <img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698757232830.png" alt="1698757232830"><br> <strong>streaming</strong> different ifmaps through the same PE set <strong>sequentially</strong><br> the filter <strong>stays constant</strong> in the set(filter reuse)</li>
<li>numbers of kernels M<br> <img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698757353178.png" alt="1698757353178"><br><strong>ifmaps reuse</strong></li>
<li>numbers of channel C<br> <img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698757366566.png" alt="1698757366566"><br> <strong>ifmaps reuse</strong><br> <strong>psums accumulation</strong></li>
</ol>
<ul>
<li><p><em><strong>时间上的合理规划</strong></em><br>for N</p>
</li>
<li><p><em><strong>Multiple 2-D Convolutions in a PE Set</strong></em><br><strong>PE 内部的并行计算</strong><br>可以使每个 PE 不仅仅只计算一个 1-D 卷积，而是<strong>同时</strong>计算 <strong>p(for filters) × q(for channels)</strong> 个 1-D 卷积，这要求：</p>
<ol>
<li>Capacity of filter spad: p × q × S</li>
<li>Capacity of ifmaps spad: q × S</li>
<li>Capacity of psums spad: p × 1</li>
</ol>
<p>for M &amp; C</p>
</li>
<li><p><em><strong>Multiple PE Sets in the PE Array</strong></em><br><strong>PE 之间的并行计算</strong><br>The PE array fits <strong>r × t</strong> PE sets in parallel that run r different channels of t different filters <strong>simultaneously</strong>.</p>
<ol>
<li>Every t sets share <strong>the same ifmap</strong> with t filters</li>
<li>Every r sets that run on r channels <strong>accumulate their psums</strong> within the PE array.</li>
</ol>
<p>再次以 AlexNet 为例，可以进行如下映射<br><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698756278238.png" alt="1698756278238"><br>可令，CONV1: t&#x3D;2, CONV3: t&#x3D;4, CONV4&amp;CONV5: r&#x3D;t&#x3D;4;</p>
</li>
<li><p><em><strong>PE array processing passes</strong></em></p>
<ol>
<li>The PE array can run multiple 2-D convolutions from up to q × r channels of p × t filters simultaneously.</li>
<li>Multiple ifmaps can also be processed sequentially through the array.</li>
</ol>
</li>
</ul>
<p>比如，一个卷积层，<strong>C&#x3D;6, M&#x3D;8, N&#x3D;4,</strong> 一个通道可处理 <strong>3 (q×r) channel</strong> 和 <strong>4 (p×t) filters</strong>. 每次传递 <strong>ifmaps 的量 n&#x3D;2</strong>，则时序可按如下安排：<br><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698761331459.png" alt="1698761331459"></p>
<p>每个 Pass 完成后，都将改组 psum 存在 GLB 中，故上例在最终完成后，<strong>GLB</strong> 将存储 <strong>m&#x3D;8</strong> 组 psum</p>
<ul>
<li><em><strong>Summary</strong></em><br><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698762095756.png" alt="1698762095756"></li>
</ul>
</li>
</ul>
<h3 id="Exploit-Data-Statistics"><a href="#Exploit-Data-Statistics" class="headerlink" title="Exploit Data Statistics"></a>Exploit Data Statistics</h3><p>ReLu(Rectified Linear Unit) function: f(x)&#x3D;max(0,x)</p>
<p>因此，会存在大量的0；</p>
<p>使用 <strong>RLC</strong> 游程编码</p>
<ul>
<li>5 bit作为最大的连续0的长度，然后直接插入下一个数据16 bit，成为 1 pair</li>
<li>3 pairs + 1 indicating bit &#x3D; 1 word (64 bit)</li>
</ul>
<p><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1698771515055.png" alt="1698771515055"></p>
<h2 id="SYSTEM-MODULES"><a href="#SYSTEM-MODULES" class="headerlink" title="SYSTEM MODULES"></a>SYSTEM MODULES</h2><h3 id="Global-Buffer"><a href="#Global-Buffer" class="headerlink" title="Global Buffer"></a>Global Buffer</h3><ul>
<li>108kB</li>
<li>the space is divided into 25 banks, each of which is a 512-b ×64-b (4 kB) SRAM</li>
<li>the remaining 8 kB (two banks of 512-b × 64-b SRAMs) of the GLB is allocated for filter weights</li>
</ul>
<h3 id="Network-on-Chip"><a href="#Network-on-Chip" class="headerlink" title="Network-on-Chip"></a>Network-on-Chip</h3><ul>
<li>Global Input Network<ul>
<li>use ID match</li>
<li>every data has a col id and row id</li>
<li>every PE has a col id and row id</li>
<li>they match by a Multicast Controller (MC)</li>
</ul>
</li>
</ul>
<p><img src="/2023/10/31/Eyeriss_An%20Energy-Efficient%20Reconfigurable%20Accelerator%20for%20Deep%20Convolutional%20Neural%20Networks/Eyeriss_AnEnergy-EfficientReconfigurableAcceleratorforDeepConvolutionalNeuralNetworks/1699116006038.png" alt="1699116006038"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/09/AI_and_ML_Accelerator_Research/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/09/AI_and_ML_Accelerator_Research/" class="post-title-link" itemprop="url">AI_and_ML_Accelerator_Research</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-10-09 02:43:04" itemprop="dateCreated datePublished" datetime="2023-10-09T02:43:04+08:00">2023-10-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-10-24 19:06:00" itemprop="dateModified" datetime="2023-10-24T19:06:00+08:00">2023-10-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BE%88%E6%B0%B4%E7%9A%84%E8%B0%83%E7%A0%94%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">很水的调研类</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p><strong>credit to</strong> 《人工智能芯片技术白皮书2018，清华大学》《AI 芯片前沿技术与创新未来，张臣雄》 《2022亿库关于中国ai芯片行业的调研》</p>
</blockquote>
<p><em><strong>当英伟达成为全球首家市值超过1万亿美元芯片公司时，便意味着AI芯片的的热潮前所未有地席卷各行各业，成为人工智能时代的最重要的物理基石——无芯片，不 AI</strong></em></p>
<h2 id="什么是-AI-芯片？"><a href="#什么是-AI-芯片？" class="headerlink" title="什么是 AI 芯片？"></a>什么是 AI 芯片？</h2><p><strong>定义</strong>：从广义上讲只要能够运行人工智能算法的芯片都叫作 AI 芯片。但是通常意义上的 AI 芯片指的是针对人工智能算法做了特殊加速设计的芯片，现阶段，这些人工智能算法一般以深度学习算法为主 (比如包括 DNN, CNN, RNN, LSTM, GAN, Transformer)</p>
<ul>
<li>研究如何设计出高效的硬件友好型的智能算法</li>
<li>研究如何将这些算法有效可靠地在芯片上实现</li>
</ul>
<p><strong>AI 芯片本身处于整个链条的中部，向上为应用和算法提供高效支持，向下对器件和电路、工艺和材料提出需求。</strong></p>
<p><img src="/2023/10/09/AI_and_ML_Accelerator_Research/1696787293538.png" alt="Fig1"></p>
<h2 id="AI-芯片的种类"><a href="#AI-芯片的种类" class="headerlink" title="AI 芯片的种类"></a>AI 芯片的种类</h2><h3 id="按类型"><a href="#按类型" class="headerlink" title="按类型"></a>按类型</h3><ol>
<li><p><strong>经过软硬件优化可以高效支持 AI 应用的通用芯片</strong></p>
<ul>
<li><p><strong>GPU</strong></p>
<p>GPU是单指令、多数据处理，采用数量众多的计算单元和超长的流水线，主要处理图像领域的运算加速。GPU是不能单独使用的，它只是处理大数据计算时的能手，必须由CPU进行调用，下达指令才能工作。为此开发的专用编程系统CUDA，能够帮助工程师们运行数万个并发线程和数百个处理器核。<br><strong>高度并行计算，软硬件协同</strong></p>
</li>
</ul>
</li>
</ol>
<p><img src="/2023/10/09/AI_and_ML_Accelerator_Research/1698144036046.png" alt="1698144036046"></p>
<ul>
<li><strong>TPU, NPU</strong></li>
</ul>
<ol start="2">
<li><p><strong>侧重加速机器学习（尤其是神经网络、深度学习）算法的芯片</strong></p>
<ul>
<li><p><strong>FPGA</strong></p>
<p>可编程逻辑门阵列，是一种“可重构”芯片，具有模块化和规则化的架构，主要包含可编程逻辑模块、片上储存器及用于连接逻辑模块的可重构互连层次结构。</p>
</li>
<li><p><strong>ASIC</strong></p>
<p>特定用户要求和特定电子系统的需要而设计、制造的集成电路。</p>
</li>
<li><p><strong>两者对比</strong><br> FPGA具有开发周期短，上市速度快，可配置性等特点，目前被大量的应用在大型企业的线上数据处理中心和军工单位。ASIC一次性成本远远高于FPGA，但由于其量产成本低，应用上就偏向于消费电子，如移动终端等领域。</p>
</li>
</ul>
</li>
</ol>
<p><img src="/2023/10/09/AI_and_ML_Accelerator_Research/1698144099559.png" alt="1698144099559"><br><img src="/2023/10/09/AI_and_ML_Accelerator_Research/1698144177110.png" alt="1698144177110"></p>
<ol start="3">
<li><p><strong>受生物脑启发设计的神经形态计算芯片</strong></p>
<ul>
<li><p><strong>类脑芯片</strong></p>
<p>类脑芯片架构是一款模拟人脑的神经网络模型的新型芯片编程架构，这一系统可以模拟人脑功能进行感知方式、行为方式和思维方式，使用的是脉冲神经网络(SNN)。如，清华大学天机系列芯片。</p>
</li>
</ul>
</li>
</ol>
<p><img src="/2023/10/09/AI_and_ML_Accelerator_Research/1698144682568.png" alt="1698144682568"></p>
<h3 id="按功能分类"><a href="#按功能分类" class="headerlink" title="按功能分类"></a>按功能分类</h3><p>根据机器学习算法步骤，可分为训练和推断两个环节：</p>
<ul>
<li><p><strong>训练 (Training)</strong></p>
<p>训练环节通常需要通过大量的数据输入，训练出一个复杂的深度神经网络模型。训练过程由于涉及海量的训练数据和复杂的深度神经网络结构，运算量巨大，需要庞大的计算规模。训练要求<strong>高精度、高吞吐量、强大的算力</strong>。目前市场上通常使用英伟达的GPU集群，Google的 TPU 来训练。</p>
</li>
<li><p><strong>推断 (Inference)</strong></p>
<p>推断环节是指利用训练好的模型，使用新的数据去“推断”出各种结论。这个环节的计算量相对训练环节少很多，但仍然会涉及到大量的矩阵运算。因此，对于众多应用场景来说，<strong>速度、能效、安全和硬件成本</strong>是考虑的因素。</p>
</li>
<li><p><strong>趋势</strong></p>
<p>在未来的 AI 应用当中，训练（学习）和推断在更多场景下会是交织在一起的。<br>推断放在边缘侧，更低延时，更安全隐私</p>
</li>
</ul>
<h3 id="按-AI-部署的位置"><a href="#按-AI-部署的位置" class="headerlink" title="按 AI 部署的位置"></a>按 AI 部署的位置</h3><p><img src="/2023/10/09/AI_and_ML_Accelerator_Research/1696788206981.png" alt="Fig2"></p>
<ul>
<li><p><strong>云端</strong></p>
<p>云端是指在数据中心或超级计算机，利用海量的数据和庞大而复杂的深度学习算法进行模型训练，也能用作推理。</p>
</li>
<li><p><strong>边缘侧</strong></p>
<p>数据中心外的设备，如自动驾驶汽车、机器人、智能手机、物联网设备等，一般用训练好的模型进行推理。</p>
</li>
<li><p><strong>趋势</strong></p>
<p>云的边界也 逐渐向数据的源头推进，未来很可能在传统的终端设备和云端设备直接出现更多的边缘设备，把 AI 处理分布在各种网络设备（比如5G的基站）中，让数据尽量实现本地处理，这样更低延时，更安全隐私。</p>
</li>
</ul>
<h2 id="AI-芯片的技术挑战"><a href="#AI-芯片的技术挑战" class="headerlink" title="AI 芯片的技术挑战"></a>AI 芯片的技术挑战</h2><ul>
<li><strong>冯·诺依曼瓶颈</strong></li>
</ul>
<p><strong>内存墙</strong>问题。</p>
<p><img src="/2023/10/09/AI_and_ML_Accelerator_Research/1696789769571.png" alt="Fig3"></p>
<ul>
<li><strong>CMOS工艺和器件瓶颈</strong></li>
</ul>
<p><strong>Dennard Scaling定律</strong>已经失效，<strong>Amdahl’s Law</strong>已经达到了极限，<strong>Moore’s Law</strong>也变得越来越难以遵循，而且成本也越来越高，特别是在功率和性能效益下降的情况下。</p>
<h2 id="AI-芯片的优化方向"><a href="#AI-芯片的优化方向" class="headerlink" title="AI 芯片的优化方向"></a>AI 芯片的优化方向</h2><h3 id="新型计算范式"><a href="#新型计算范式" class="headerlink" title="新型计算范式"></a>新型计算范式</h3><p>目前，AI芯片主要用于DNN模型的训练和推理，计算量非常大，故需从硬件友好方面考虑对算法的改进，降低计算成本。</p>
<ul>
<li><p><strong>降低数值精度的量化技术</strong></p>
<p>减少位宽，实现一个精度与硬件开销的 trade-off</p>
</li>
<li><p><strong>压缩网络规模、修剪网络</strong></p>
<p>在DNN计算中，有许多值是0或接近于0或重复的值，将这些值输入一个MAC进行乘积累加运算没有意义，浪费资源，因此需要压缩和剪枝。</p>
</li>
<li><p><strong>二值、三值神经网络</strong></p>
<p>BNN: Binary Neural Network</p>
<p>TNN: Ternary Neural Network</p>
</li>
<li><p><strong>增加和利用网络稀疏性</strong></p>
<p>稀疏化数据流感知</p>
</li>
</ul>
<h3 id="架构的设计和优化"><a href="#架构的设计和优化" class="headerlink" title="架构的设计和优化"></a>架构的设计和优化</h3><ul>
<li><p><strong>CPU+GPU+DSA异构并行计算</strong></p>
</li>
<li><p><strong>存算感一体新架构</strong></p>
<p>有望突破冯·诺依曼架构，存内运算、近内存计算、基于新型存储器的人工神经网络、生物神经网络，等等。</p>
</li>
</ul>
<h3 id="电路的设计和优化"><a href="#电路的设计和优化" class="headerlink" title="电路的设计和优化"></a>电路的设计和优化</h3><ul>
<li><p><strong>FPGA Overlay技术</strong></p>
<p>为了提高FPGA的开发效率、更好的利用FPGA的逻辑资源、方便FPGA的大规模部署和应用，需要将FPGA进行一定程度的逻辑抽象，使顶层用户不必太多关注于FPGA硬件逻辑的实现方式与细节。</p>
</li>
<li><p><strong>模数混合电路设计MAC</strong></p>
</li>
<li><p><strong>软件定义芯片</strong></p>
<p>可重构计算技术允许硬件架构和功能随软件变化而变化，具备处理器的灵活性和专用集成电路的高性能和低功耗，被认为是突破性的下一代集成电路技术</p>
<p><img src="/2023/10/09/AI_and_ML_Accelerator_Research/1696789666625.png" alt="Fig4"></p>
</li>
</ul>
<blockquote>
<p>2023年10月24日，按照李炎原来的说法“调查一下该方向的历史发展啊，研究现状啊”进行了pre，结果被李炎说太宽泛有点水。本来就是自己没说清楚。但是，我觉得他有句话说得对，“像是在半导体大会上做报告”，这是不是说我有做企业高管领导的潜质捏？:stuck_out_tongue_winking_eye:</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/" class="post-title-link" itemprop="url">截断方法的近似乘法器</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-10-05 21:33:05" itemprop="dateCreated datePublished" datetime="2023-10-05T21:33:05+08:00">2023-10-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-10-07 19:40:07" itemprop="dateModified" datetime="2023-10-07T19:40:07+08:00">2023-10-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%B8%80%E8%B5%B7%E8%AF%BB%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">一起读论文</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>论文链接：<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8605690">A
Probabilistic Prediction-Based Fixed-Width Booth Multiplier for
Approximate Computing</a></p>
<p>作者：Ya juan He , Member, IEEE, Xilin Yi, Graduate Student Member,
IEEE, Ziji Zhang, Student Member, IEEE, Bin Ma, and Qiang Li , Senior
Member, IEEE</p>
<h2 id="abstract">ABSTRACT</h2>
<p>在电路性能与计算结果精确度的折中背景下，该论文提出了一种基于概率预测的固定宽度Booth乘法器</p>
<h2 id="introduction">INTRODUCTION</h2>
<ul>
<li>近似计算越来越重要，应用场景也越来越多</li>
<li>乘法是十分基本的操作，故乘法器改良设计是非常必要的</li>
<li>目前乘法器应用基本上无需过高精度，故固定位宽乘法器保守青睐</li>
<li>直接截断乘法器硬件开销最低，但精度最差；后截断乘法器精度最高，但硬件开销也最高</li>
<li>该论文为实现两者有效折中，提出一种新的基于条件概率的截断补偿算法，并以此设计了固定宽度Booth乘法器</li>
<li>介绍了论文后续章节的大概内容</li>
</ul>
<h2 id="fixed-width-booth-multiplier-fwbm">FIXED-WIDTH BOOTH MULTIPLIER
(FWBM)</h2>
<h3 id="booth-encodeing">Booth Encodeing</h3>
<ul>
<li>设一个 <span class="math inline">\(n\)</span> bit 的数 <span class="math inline">\(y\)</span> 为</li>
</ul>
<p><span class="math inline">\(\begin{aligned} y &amp;=
y_{n-1}y_{n-2}y_{n-3}\dotsc y_{1}y_{0}\\\\
&amp;=-y_{n-1}*2^{n-1}+y_{n-2}*2^{n-2}+y_{n-3}*2^{n-3}+\dotsc
+y_{1}*2^{1}+y_{0}*2^{0}\\\\
&amp;=-y_{n-1}*2^{n-1}+\sum_{i=0}^{n-2}y_{i}*2^{i}
\end{aligned}\)</span></p>
<ul>
<li>基于2的booth编码(令 <span class="math inline">\(y_{-1}=0\)</span>)</li>
</ul>
<p><span class="math inline">\(\begin{aligned} y &amp;=
(y_{n-2}-y_{n-1})*2^{n-1}+(y_{n-3}-y_{n-2})*2^{n-2}+\dotsc
+(y_{0}-y_{1})*2^{1}-y_{0}*2^{0}\\\\ &amp;=
\sum_{i=0}^{n-1}(y_{i-1}-y_{i})*2^{i} \end{aligned}\)</span></p>
<ul>
<li>基于4的booth编码(令 <span class="math inline">\(y_{-1}=0\)</span>)</li>
</ul>
<p><span class="math inline">\(\begin{aligned} y &amp;=
(-2y_{n-1}+y_{n-2}+y_{n-3})*2^{n-2}+(-2y_{n-3}+y_{n-4}+y_{n-5})*2^{n-4}+\dotsc
+(-2y_{3}+y_{2}+y_{1})*2^{2}+(-2y_{1}+y_{0})*2^{0}\\\\ &amp;=
\sum_{k=0}^{\frac{n}{2}-1}(-2y_{2k+1}+y_{2k}+y_{2k-1})*2^{2k}\\\\ &amp;=
\sum_{k=0}^{\frac{n}{2}-1}d_{k}*2^{2k} \end{aligned}\)</span></p>
<p>即将三元组 <span class="math inline">\((y_{2k+1},y_{2k},y_{2k-1})\)</span> 映射至 <span class="math inline">\(d_{k}\)</span>，</p>
<p><span class="math inline">\(c_{k}\)</span> 为校正位，即若 <span class="math inline">\(d_{k}\)</span>
为负数，则根据补码规则，将乘积取反后需要加上1</p>
<p>所有的8种情况如下表： <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696487355293.png" alt="1696487355293"></p>
<h3 id="full-accuracy-productsp_a">Full Accuracy Products(<span class="math inline">\(P_{A}\)</span>)</h3>
<p><span class="math inline">\(\begin{aligned} P_{A} &amp;=
X(x_{n-1}x_{n-2}x_{n-3}\dotsc x_{1}x_{0})*Y(y_{n-1}y_{n-2}y_{n-3}\dotsc
y_{1}y_{0})\\\\
&amp;=(-x_{n-1}*2^{n-1}+\sum_{i=0}^{n-2}x_{i}*2^{i})*\sum_{k=0}^{\frac{n}{2}-1}d_{k}*2^{2k}
\qquad d_{k}=±0,±1,±2 \end{aligned}\)</span></p>
<p>设 <span class="math inline">\(PP_{k}\)</span> 为第 <span class="math inline">\(k\)</span> 个部分积(partial product)，<span class="math inline">\(p_{k,i}\)</span> 为 <span class="math inline">\(PP_{k}\)</span> 的第 <span class="math inline">\(i\)</span> 位 假设 <span class="math inline">\(X,Y\)</span> 皆为 8 bit，即 <span class="math inline">\(X(x_{7}x_{6}x_{5}\dotsc
x_{1}x_{0})\)</span>，则全部6种情况如下 <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696487337317.png" alt="1696487337317">
整个乘法的过程则如下图（假设 n 为偶数） <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696487704909.png" alt="1696487704909"></p>
<ul>
<li><span class="math inline">\(X,Y\)</span> 皆为 n 位小数，<span class="math inline">\(PP_{k}\)</span> 则有 n+1 位，使用双符号位，则有
n+2 位，</li>
<li><span class="math inline">\(c_{k}\)</span> 为 <span class="math inline">\(PP_{k}\)</span> 的补码校验位，</li>
<li>每次部分积进行累积时都需要乘2，故须左移两位对齐，最后一项部分积为
对应的是<span class="math inline">\(2^{n-2}\)</span>，故 <span class="math inline">\(p_{\frac{n}{2}-1,0}\)</span> 对齐的是 <span class="math inline">\(2^{-n+n-2}=2^{-2}\)</span> 位</li>
<li><span class="math inline">\(P_{k}\)</span> 为最终积的第 <span class="math inline">\(k\)</span> 位，<span class="math inline">\(AP\)</span>(accurate part)占 <span class="math inline">\(n\)</span> bit，<span class="math inline">\(TP\)</span>(truncation part)占 <span class="math inline">\(n\)</span> bit，<span class="math inline">\(P_T\)</span> 为所需要的最终近似积，<span class="math inline">\(\sigma\)</span> 为根据 <span class="math inline">\(TP\)</span> 估计得到误差 <span class="math inline">\(P_{T}=AP+\sigma\)</span></li>
<li><strong><span class="math inline">\(\sigma\)</span>
的算法是最关键的，</strong> <span class="math inline">\(\sigma(PTBM) =
TP + 1*2^{-1}\)</span> <span class="math inline">\(\sigma(DTBM) =
0\)</span> 上述两种算法分别代表了两个极端：最高精度与最低硬件开销
<strong>因此设计 <span class="math inline">\(\sigma\)</span>
算法的设计本质是考虑一个 trade-off</strong></li>
</ul>
<h2 id="proposed-probabilistic-prediction-formula-ppf">PROPOSED
PROBABILISTIC PREDICTION FORMULA (PPF)</h2>
<p>下面是一种基于条件概率的<span class="math inline">\(\sigma\)</span>算法 <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696489498284.png" alt="1696489498284">
<span class="math inline">\(\sigma =
TP_{major}+\mu(TP_{minor})+a_{round}*2^{-2}\)</span></p>
<h3 id="mutp_minor"><span class="math inline">\(\mu(TP_{minor})\)</span></h3>
<p>分为三部分来计算数学期望 <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696490023809.png" alt="1696490023809"></p>
<ul>
<li><span class="math inline">\(S_I\)</span></li>
</ul>
<p>由于<span class="math inline">\(p_{k,i}\)</span>实际上只取0或1，并假设概率相同，皆为<span class="math inline">\(\frac{1}{2}\)</span> 由于<span class="math inline">\(PP_{0}\)</span>在计算时，<span class="math inline">\(y_{-1}=0\)</span>，故仅有四种情况，<span class="math inline">\(d_{0}=0,-1,1,-2\)</span></p>
<p><span class="math inline">\(\begin{aligned}
E\{p_{0,0}\}&amp;=\sum_{j=0,±1,-2}1*P\{p_{0,0}=1|d_{0}=j\}*P\{d_{0}=j\}\\\\
&amp;=0*\frac{1}{4}+\frac{1}{2}*\frac{1}{4}+\frac{1}{2}*\frac{1}{4}+1*\frac{1}{4}\\\\
&amp;=\frac{1}{2} \end{aligned}\)</span></p>
<p><span class="math inline">\(\begin{aligned}
E\{p_{0,i}\}&amp;=\sum_{j=0,±1,-2}1*P\{p_{0,i}=1|d_{0}=j\}*P\{d_{0}=j\}\\\\
&amp;=0*\frac{1}{4}+\frac{1}{2}*\frac{1}{4}+\frac{1}{2}*\frac{1}{4}+\frac{1}{2}*\frac{1}{4}\\\\
&amp;=\frac{3}{8}\\\\ &amp;(i=1,2,\dotsc,n-2) \end{aligned}\)</span></p>
<p>其余情况下，8种情况都存在，<span class="math inline">\(d_{i}=±0,±1,±2\)</span></p>
<p><span class="math inline">\(\begin{aligned}  E\{p_{k,0}\}&amp;=\sum_{j=0,-0,1,-1,2,-2}1*P\{p_{k,0}=1|d_{k}=j\}*P\{d_{k}=j\}\\\\  &amp;=0*\frac{1}{8}+1*\frac{1}{8}+\frac{1}{2}*\frac{1}{4}+\frac{1}{2}*\frac{1}{4}+0*\frac{1}{8}+1*\frac{1}{8}\\\\  &amp;=\frac{1}{2}\\\\  &amp;(k=1,2,\dotsc,\frac{n}{2}-2)  \end{aligned}\)</span></p>
<p><span class="math inline">\(\begin{aligned}  E\{p_{k,i}\}&amp;=\sum_{j=0,-0,1,-1,2,-2}1*P\{p_{k,i}=1|d_{k}=j\}*P\{d_{k}=j\}\\\\  &amp;=0*\frac{1}{8}+1*\frac{1}{8}+\frac{1}{2}*\frac{1}{4}+\frac{1}{2}*\frac{1}{4}+\frac{1}{2}*\frac{1}{8}+\frac{1}{2}*\frac{1}{8}\\\\  &amp;=\frac{1}{2}\\\\  &amp;(k=1,2,\dotsc,\frac{n}{2}-2;i=1,2,\dotsc,n-2-2k)  \end{aligned}\)</span></p>
<ul>
<li><span class="math inline">\(S_{II}\)</span></li>
</ul>
<p><span class="math inline">\(\begin{aligned}  E\{c_0\}&amp;=\sum_{j=0,±1,-2}1*P\{c_{0}=1|d_{0}=j\}*P\{d_{0}=j\}\\\\  &amp;=0*\frac{1}{4}+0*\frac{1}{4}+1*\frac{1}{4}+1*\frac{1}{4}\\\\  &amp;=\frac{1}{2}  \end{aligned}\)</span></p>
<p><span class="math inline">\(\begin{aligned}  E\{c_{k}\}&amp;=\sum_{j=0,-0,1,-1,2,-2}1*P\{c_{k}=1|d_{k}=j\}*P\{d_{k}=j\}\\\\  &amp;=0*\frac{1}{8}+1*\frac{1}{8}+0*\frac{1}{4}+1*\frac{1}{4}+0*\frac{1}{8}+1*\frac{1}{8}\\\\  &amp;=\frac{1}{2}\\\\  &amp;(k=1,2,\dotsc,\frac{n}{2}-2)  \end{aligned}\)</span></p>
<ul>
<li><span class="math inline">\(S_{III}\)</span></li>
</ul>
<p>设 <span class="math inline">\(p_{\frac{n}{2}-1,0}\)</span> 与 <span class="math inline">\(c_{\frac{n}{2}-1}\)</span> 的本位和为 <span class="math inline">\(\beta\)</span>，进位为 <span class="math inline">\(\theta\)</span>，所有情况如下表 <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696493223315.png" alt="1696493223315">
于是， <span class="math inline">\(\theta =
\overline{(x_{0}\cdot(y_{n-2}\oplus y_{n-3}))}\cdot y_{n-1}\)</span>
<span class="math inline">\(E\{\beta\}=0\cdot
\frac{1}{2}+\frac{1}{2}\cdot \frac{1}{2}=\frac{1}{4}\)</span></p>
<ul>
<li><span class="math inline">\(\mu(TP_{minor})\)</span></li>
</ul>
<p><span class="math inline">\(\begin{aligned}
\mu(TP_{minor})&amp;=\sum_{k=0}^{\frac{n}{2}-2}\sum_{i=0}^{\frac{n}{2}-2-2k}E\{p_{k,i}\}\cdot
2^{i+2k-n} + \sum_{k=0}^{\frac{n}{2}-2}E\{c_{k}\}\cdot 2^{2k-n} +
E\{\beta\}\cdot 2^{-2} + \theta \cdot 2^{-1}\\\\ &amp;=[\frac{3}{8}\cdot
(2^{-2}+\dotsc +2^{-n+1}) + \frac{1}{2} \cdot
(2^{-n}+2^{-n})]+(\frac{n}{4}-\frac{7}{8}+\theta)\cdot 2^{-1}
\end{aligned}\)</span></p>
<p>将<span class="math inline">\(p_{0,0}, c_0\)</span>忽略不计，<span class="math inline">\(\frac{1}{2}\)</span> 替换为 <span class="math inline">\(\frac{3}{8}\)</span></p>
<p><span class="math inline">\(\begin{aligned}
\mu(TP_{minor})&amp;\approx [\frac{3}{8}\cdot (2^{-2}+\dotsc +2^{-n+1})
+ \frac{3}{8} \cdot
(2^{-n}+2^{-n})]+(\frac{n}{4}-\frac{7}{8}+\theta)\cdot 2^{-1}\\\\
&amp;=(\frac{n}{4}-\frac{1}{2}+\theta)\cdot 2^{-1}
\end{aligned}\)</span></p>
<h3 id="alpha_round"><span class="math inline">\(\alpha_{round}\)</span></h3>
<p><span class="math inline">\(\alpha_{round}=1\)</span></p>
<h3 id="tp_major"><span class="math inline">\(TP_{major}\)</span></h3>
<p><span class="math inline">\(TP_{major}=(p_{0,n-1}+p_{1,n-3}+\dotsc
+p_{\frac{n}{2}-1,1})\cdot 2^{-1}\)</span></p>
<h3 id="sigma"><span class="math inline">\(\sigma\)</span></h3>
<p><span class="math inline">\(\sigma =
TP_{major}+(\frac{n}{4}+\theta)\cdot 2^{-1}\)</span></p>
<h3 id="如何处理fracn4">如何处理<span class="math inline">\(\frac{n}{4}\)</span>?</h3>
<p>进行预讨论，将其加进 <span class="math inline">\(PP_{0|AP}\)</span>
中</p>
<ul>
<li><p>n是8的倍数 <span class="math inline">\(\frac{n}{4}\cdot2^{-1}=\frac{8k}{4}\cdot
2^{-1}=k\)</span> 将k转换为二进制后，与 <span class="math inline">\(\overline{s_0}s_0s_0\)</span> 相加，得到新的<span class="math inline">\(PP_{0|AP}\)</span> <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696496927372.png" alt="1696496927372"></p></li>
<li><p>n是4的倍数但不是8的倍数 思路同上，列出部分n的取值如下表所示 <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696497142368.png" alt="1696497142368"></p></li>
<li><p>n既不是4的倍数也不是8的倍数的其余偶数
四舍五入到最近的8的倍数</p></li>
</ul>
<h2 id="probabilistic-prediction-based-fwbm-design">PROBABILISTIC
PREDICTION BASED FWBM DESIGN</h2>
<h3 id="architecture-of-proposed-fwbm">Architecture of Proposed
FWBM</h3>
<figure>
<img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696502973412.png" alt="1696502973412">
<figcaption aria-hidden="true">1696502973412</figcaption>
</figure>
<h3 id="hardware-implementation">Hardware Implementation</h3>
<p>以n=8为例，FA: Full Adder, HA: Half Adder <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696503066348.png" alt="1696503066348"></p>
<h2 id="performance-evaluation">PERFORMANCE EVALUATION</h2>
<h3 id="error-performance">Error Performance</h3>
<p>平均误差与最坏情况都非常的必要 <span class="math inline">\(\overline{\epsilon}=\frac{E\{P_{A}-P_{T}\}}{2^n}\)</span>
<span class="math inline">\(\overline{|\epsilon|}=\frac{E\{|P_{A}-P_{T}|\}}{2^n}\)</span>
<span class="math inline">\(\epsilon_{max}=\frac{max(|P_{A}-P_{T}|)}{2^n}\)</span></p>
<figure>
<img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696505787256.png" alt="1696505787256">
<figcaption aria-hidden="true">1696505787256</figcaption>
</figure>
<h3 id="circuit-performance">Circuit Performance</h3>
<p>包括area cost, propagation delay, power dissipation三方面
作不同电路的对照时，要保证下列要素的一致性：</p>
<ul>
<li>CMOS standard-cell library</li>
<li>EDA工具（包括生成与仿真）如：the Synopsys Design Compiler (DC)</li>
<li>Voltage, Temperature <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696505867596.png" alt="1696505867596"></li>
</ul>
<h3 id="design-efficiency-analysis">Design Efficiency Analysis</h3>
<p>提供一个综合的指标 <span class="math inline">\(P_{AED\epsilon}=Area(A)\times Energy(E)\times
Delay(D)\times |\epsilon|\)</span> <img src="/2023/10/05/%E6%88%AA%E6%96%AD%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%91%E4%BC%BC%E4%B9%98%E6%B3%95%E5%99%A8/1696505842844.png" alt="1696505842844"></p>
<h3 id="chip-implementation">Chip Implementation</h3>
<p>有这么多钱去流片？！</p>
<h3 id="application-to-2-d-dct-computations">Application to 2-D DCT
Computations</h3>
<p>进一步说明其应用性并测试其真实性能</p>
<h2 id="conclusion">CONCLUSION</h2>
<p>其实与introduction差不多，只不过对电路的实现框架再具体一点总结，并再次交代性能的数据。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/30/%E5%AE%9E%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/30/%E5%AE%9E%E4%B9%A0/" class="post-title-link" itemprop="url">2023暑期实习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-30 18:54:56" itemprop="dateCreated datePublished" datetime="2023-07-30T18:54:56+08:00">2023-07-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-10-05 21:28:20" itemprop="dateModified" datetime="2023-10-05T21:28:20+08:00">2023-10-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="2023-7-31"><a href="#2023-7-31" class="headerlink" title="2023.7.31"></a>2023.7.31</h2><ul>
<li><strong>2023年7月16日11点13分，浙江绍兴，上虞南站。</strong><br>兴奋、期待、忐忑……不知道应该用什么样的词语来形容我此刻的心情，这是我第一次，孤身一人，来到一座陌生的城市。公司环境会是怎样，新舍友怎么样，同事们怎么样，工作究竟会不会做……无数的想法充斥着我的脑海里，但是这是一次全新的体验，因为这意味着第一次走出学校，走进社会。<br>在火车站等公交车等了半小时竟然都等不到一辆，我只好打车来到中芯集成，并在一番折腾后终于拖着重重的箱子，满头大汗并忐忑地敲开了新宿舍的门。<br><strong>“hi~，我是你们的新舍友。”</strong></li>
<li>我的三个新舍友都是研二的学生，也是暑假来实习的。由于他们都比我大了好几岁，并且我本身就比较内向与社恐，刚开始跟他们交流的时候就显得有点胆怯（可能不止一点），尽管他们都非常的友善。当他们惊叹于我大二就来实习的时候并且是fdu的学生时，我连忙摇头摆手说自己很菜。我从小到大都是这样，过于内敛，害怕被人夸赞，但实际上虽然谦虚是好事，可是有时候就应该大方自信一点。反思过后，我认为再来一次的话应该要这样子回答：<br><em>Q:大二就来实习了吗？</em><br>*A:对，早点来实习长长见识！<br><em>Q:你是fdu的啊？牛逼</em><br><em>A:哈哈哈还行吧，以后还得向你们多多请教呢！</em></li>
<li>我被安排到的部门是 <em>Technique &amp; Marketing Centre - Design Enablement - Spice Model - HV&#x2F;Anolog SPICE</em>，职位是 <em>HVS</em>。带我的周扬师兄，以及其他所有同事都很热情友善，整个小组氛围也很好。但可能还是因为年龄差的原因，并且我觉得我没有一点工作经验以及对于能否做好实习的工作缺少底气，我和同事们交流的时候还是放不开，不太自然（自己听自己的声音都能明显感觉得到）。唉，树安，下次自信一点，展示一下fdu高材生魅力，how say！</li>
<li>第一个星期还没配好电脑，于是自己看了看书，复习了一下半物和模电，在b站上学了点强化学习；第二个星期终于等来了电脑，但是周扬师兄让我先看下培训资料，讲的是一些器件原理，仿真、测试原理和流程，公司的工艺平台，等等，还是学到不少东西。</li>
<li>星期五晚上和同事们一起出去吃顿海鲜自助餐😋😆<br><strong>明天，八月！</strong></li>
</ul>
<h2 id="2023-8-6"><a href="#2023-8-6" class="headerlink" title="2023.8.6"></a>2023.8.6</h2><ul>
<li>上一周学会了如何用hspice仿真电路，学会了看懂一点点网表并且尝试自己写了一下（其实不知道对不对），学会了看版图，学会了Resistance Extraction的流程。In my opinion，写网表和绘版图还是有点技术含量的，但是学会用excel绘图也还是有用的（还没学，下周研究下）。其实有很多测试的原理也没有完全搞懂，但是其实不是很重要，因为这里的很多工作都是非常完善的，无需从原理研究起来，并且来实习也不是为了学原理和技术。</li>
<li>对于待人接物（对于陌生人和一些不是很熟很交心的朋友）的方式又有了深刻的体会：首先要自信，这意味着，气场要强大，有自己独立思考的能力，有自己的想法，不能显露出自己的情绪，要时刻保持冷静和理性；其次是尊重别人，微笑，gentleman。</li>
<li>据我猜测，在这个岗位上的正式员工，应该是一两万一个月左右，工作不会很多，压力不会很大，有一定的技术含量，这样一个年能有个二十来万。实习摸鱼的时候，我在思考这样的问题，假设以后毕业出来打工，一年能有五十万，虽然确实已经挺好了，但还是无法十年内全款买房。怎么样，才能实现真正的跨越？我目前想到的只有以下三种方向：<strong>创业&#x2F;从工程师到高管&#x2F;投资</strong>（当然应该在有稳定的较好的工作的基础之上）。前两种的一个前提是，技术本身要够硬，专业知识够强；前两者都要具备创新能力，并且创业更需要创新。第二种则需要很好的管理能力。创业难度大，风险高，需要技术与机遇，综合考虑人脉、市场等因素；而成为企业高管，需要提升自己全方位能力。投资暂未了解。</li>
<li>ok，上面一段都是瞎jb讲的。</li>
<li>今天是tfboys十周年之约，虽然我也不完全算是四叶草，但是我感觉他们（无论是团体还是个人）确实都有很多很好听的歌，并且有些歌真的让人充满能量（比如样YOUNG）。十年之前，我也没想到十年之后是怎么样的，我现在也无法设想十年之后，这个世界，身边的人，自己，还有tfboys，都会变成怎么样。</li>
</ul>
<h2 id="2023-8-11"><a href="#2023-8-11" class="headerlink" title="2023.8.11"></a>2023.8.11</h2><p>实习终于结束了！明天就回家了！前两天写了篇实习报告给赵总看，听说还被赵总传阅了，树安你是有点东西的。</p>
<h2 id="实习报告"><a href="#实习报告" class="headerlink" title="实习报告"></a>实习报告</h2><h3 id="实习记录"><a href="#实习记录" class="headerlink" title="实习记录"></a>实习记录</h3><ul>
<li>2023年7月18日，我来到了绍兴中芯集成电路制造股份有限公司，开启了我人生中的第一次实习。</li>
<li>7月18日是实习报道日，办理入职手续和参加新员工培训，培训内容包括对公司介绍、总务情况、消防安全等等。</li>
<li>7月19日，我来到我的工作岗位上正式开始我的实习之旅。我的部门是 Technology &amp; Marketing Center - Design Enablement - SPICE Modeling - HV&#x2F;Analog SPICE，岗位是 HVS，即负责器件的 SPICE 测试，仿真与建模。一开始还没有申请到电脑，于是我看了两三天书，复习了一下半导体器件原理，听了一些培训讲座。</li>
<li>7月25日申请到电脑后，看了几天公司内部的培训资料，ppt，平台手册等，了解了整个岗位的基本任务和操作。</li>
<li>7月28日，给电脑安装好工作需要用到的软件后，终于可以开始动手操作，边做边学，如用 Hspice 仿真，进行 Resistor 参数的抽取建模，等等。</li>
<li>实习期间，还到实验室观摩了带教师兄进行WAT测试，亲眼看到了晶圆的样子。每周开例会我也都参与了，深入理解了我们SPICE MODEL组，甚至可以说是整个公司的工作，皆是以客户需求为核心，以市场为导向。</li>
</ul>
<h3 id="实习收获"><a href="#实习收获" class="headerlink" title="实习收获"></a>实习收获</h3><ol>
<li>扎实的学科基础是做好实习工作的前提条件。比如，进行WAT测试，当然得先知道要进行测试的电学特性都有哪些，并且需要非常熟悉各种半导体器件(如MOS，BIT，Diode等)的基本原理，这样在进行分析与测试时才能熟练地运用上去。这些知识都是在学校里习得的，也就是说专业知识一定要扎实，并且沉淀为自身的“常识”。</li>
<li>但实习时仅靠在学校里学过的知识是远远不够的，因为工作是一种对知识的运用，并且还会涉及许许多多没有学过的知识。这也是为什么公司要对新员工进行入职的培训。但是工作中会遇到非常多的问题，单靠培训获得的知识也是难以 handle 的，这就需要在整个工作中都要具备汲取新知识的能力，边做边学，学以致用。</li>
<li>实习第一天，我对于做些什么，为什么要这样做，应该怎么做，没有一点概念。但慢慢的，我逐渐搞明白了这三个问题。从逆向的逻辑来看：我们要做的是SPICE Model的仿真与建模，因为这是芯片设计到物理实现的重要桥梁：芯片设计好后进行物理实现要用到的元器件等，由PDK工具包来提供，而产生PDK前的关键步骤就是进行SPICE Modeling，包括器件的有关方程与参数。那么我们该如何完成这项工作呢？这就需要获得两部分数据：理想的 target 与实测的 value，然后再进行数据分析、拟合并建模。这让我明白了，无论是实习工作，还是处理其他的事情，都需要搞清楚，what, why, how。</li>
<li>Communication &amp; Corporation。小组内部每周会开一次例会，总结上周工作，提出一些新问题，并布置下周任务；平时同事们也经常探讨问题，发邮件、发信息、打电话联系其他部门的同事，以更好地进行团队合作与工作上的协调。Partnership，是公司文化中四大理念之首，这也是我这段实习中感觉最有收获的地方。因为我一直以来都不太擅长与人沟通，甚至害怕跟陌生人交流。而这次实习给了我一个改变自己的机会，慢慢走出我的 comfort zone，去认识新同事，去向比我年纪大、经验丰富的前辈请教。总之，我明白了待人接物除了要谦虚礼貌外，更要充满自信。当然我还需要更多地锻炼，学习如何更有效地与人沟通交流，进一步学习如何管理好一个团队。</li>
<li>当然，在整个实习和培训过程中还学习了许多学术上的知识和技能（详见实习笔记部分）：<ul>
<li>对半导体整个产业链、芯片设计制造封装整个流程，有了更深的认识；</li>
<li>深入了解了公司的Smec BCD_Platform, 体验了实验室 WAT 测试过程 ，学会了使用hspic进行仿真，KLayout查看版图；</li>
<li>基本 Device 的电学性能与测试原理，网表、版图的基础知识；</li>
<li>许许多多的集成电路英文专有名词。</li>
</ul>
</li>
</ol>
<h3 id="实习感想"><a href="#实习感想" class="headerlink" title="实习感想"></a>实习感想</h3><ol>
<li>人生中第一次到大厂实习，真的非常开心。特别是当我拿到工牌、坐上属于自己工位的那一刻，顿时感到自身价值倍增。</li>
<li>这次实习见识到了一个大公司究竟是怎么样的：首先在部门设计上，大概分为 TMC, Supporting Center, 运营中心，等等，分工明确，各司其职，并且内部还分为多个级别，并有相对应的主管；其次是OA系统，支撑着各种事务的正常运作；然后是部门之间的沟通方式多渠道，电脑端 Skype for Business 即使通讯，Outlook 邮件联系，手机端企业微信，还有平时在部门里面能用座机打电话联络，这些都使得一个拥有5000多名员工的大公司内部能够进行正常有效便捷的交流；而最令我印象深刻的是，公司的保密、信息安全工作做得相当严格，个人的笔记本、平板电脑都不能带进工作区域。总而言之，来到国际大公司、大厂上班给我带来的感觉还是相当的震撼。</li>
<li>公司的饭堂算是比较实惠，菜的品种也较为丰富，价格与复旦饭堂差不多，口味也相当不错。宿舍环境超好，四人间比复旦的宽敞多了，还有独立卫浴，干净舒适，唯一的缺点是没有热水饮用。公司的其他硬件设施也非常完备，但是在环境方面，每到傍晚六点多左右，厂房排出来的废气带着一些异味，空气质量下降，感觉再吸多几个星期会少活那么个几年。</li>
<li>新人入职培养方面，公司的培训计划与一对一的老带新制度可谓是非常完善和周到，能让新员工快速入门与适应工作。但我觉得培训时间还是太长了点，坐我旁边的一位老哥是新来的应届毕业生，入职一个月了还没什么事情做，还在培训阶段，看各种资料。其实我觉得这个岗位的话一两个星期基本上就能掌握一些核心的原理与操作了，很快就能上手干活，其它的完全可以边做边学。</li>
<li>在实习中发生了一件让我比较难受的事情。办理离职申请手续需要经过比较多的审核，整个流程很长比较慢，这很正常。但是如果它卡在了一个节点三四天不动，就让人很难受。8月4号13:02:19到信息安全中心进行审核，8月7日下午17:00还没审核完。于是我联系了对应负责人询问情况，问他是否可以快一点，他答应帮我催促一下。第二天早上9点半，审核还是卡在那里一动不动，于是我又在 Skype for business 上发消息给负责人，想问一下审核进行得怎么样。最让人气愤的是，这名负责人状态是在线at work，但是并没有回复消息。我又发了好几条信息恳求他能否帮忙催促一下，并且提出希望他看到信息可以回复一下，结果还是没有答复。我在企业微信和Outlook上联系他依旧不回。我还真不信他这么忙没有看到消息，于是我在OA系统上找到他的主管，和他说明了情况，请他帮忙联系一下。幸运的是那名主管人比较好，立刻在企业微信里面把我们三人拉了个群，并且 @ 那名负责人，那人立刻出现回复说好，并在不到三十分钟内就完成了审核。这很明显就是那人拖着不去审核，明明几十分钟就能完成的事情拖了好几天，办事效率极其低下，如果不是找到主管督促都不知道还要拖几天；关键是态度极差，之前答应说帮忙催促显然就是敷衍，而且看到消息不回素质非常差；这可能也是公司比较大的缘故吧，监管不到位，导致总会有人做事拖拖拉拉的，一定程度上降低了整个公司的办事效率。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总之，非常感激这段实习经历，给我带来的学习上、能力上、生活上的各种收获，得到一次从未有过的人生体验。衷心感谢师兄赵奇总经理给予这样一次提升自我的机会！祝愿中芯集成越做越好！</p>
<h3 id="附：实习笔记"><a href="#附：实习笔记" class="headerlink" title="附：实习笔记"></a>附：实习笔记</h3><h4 id="PDK"><a href="#PDK" class="headerlink" title="PDK"></a>PDK</h4><ul>
<li>PDK(Process Design kit) is the bridge between foudary and Design house</li>
<li>PDK &#x3D; SPICE MODEL + Pcell(Parameterized Cell Library) + Layout Designs + RC&#x2F;PEX(parastic Resistance and Capacitance extraction)</li>
</ul>
<h4 id="SPICE-Model"><a href="#SPICE-Model" class="headerlink" title="SPICE Model"></a>SPICE Model</h4><ol>
<li>SPICE Modeling flow:<br>Testkey Designs(BaseTK(DC数据), ROTK, AC-Cox&#x2F;CGC TK) -&gt; T&#x2F;O and Production -&gt; Measurement(WAT) -&gt; Curve Fitting -&gt; On-target Fitting(LDE Fitting, flicker(1&#x2F;f) noise fitting, mismatch Fitting, corner Fitting) -&gt; Statistic Fitting -&gt; Model Document -&gt; Model release</li>
<li>SPICE Mdodel content: a series of equation + corresponded parameters<br>eg. MOS model equation + MOS model card</li>
</ol>
<h5 id="WAT-Wafer-Acceptance-Test-or-E-Test"><a href="#WAT-Wafer-Acceptance-Test-or-E-Test" class="headerlink" title="WAT(Wafer Acceptance Test or E-Test)"></a>WAT(Wafer Acceptance Test or E-Test)</h5><ol>
<li>Inline WAT（后段工艺）or Final WAT（全部工艺完成）</li>
<li>测量的是：DUT(Device under Test)<br>Active Device: MOS BJT Diode<br>Passive Device: RS RC Capacitor<br>Process: Isolation Bridge</li>
<li>the relationship among WAT, SPICE Model, PDK:<br>TS Device provide golden wafer -&gt; Spice Testing(WAT) -&gt; Spcie Modeling -&gt; Generate PDK -&gt; CTM IP design in (pre&#x2F;post-simulation)</li>
<li>测试程式：WAT CIM系统 （Test + Prober）<br>Tester: tst(Test condition), lim(Spec), waf(Site location), die(Test key location), prb(PIC to Channel Mapping)<br>Prober: Probe Card Alignment, Wafer Alignment</li>
<li>测试结构和参数<br>Resistor: RS(Rs Rskv), RC(Rc Rckv)<br>Capacitor:Cap(MIM MOM Varactor), Ileak, Vbd<br>Inductor, Contimuity, Bridge, Isolation(介质隔离, LOCOS隔离, STI, Guard Ring, DNWring, Sealring)<br>Diode: Cg, Leak<br>BJT: Vbe β Vce<br>Mosfet: Vt Id Ioff Bvds Swing Cgg Cgc Cgs DIBL Vhg Rout .etc<br>Cmos: Vtc<br>Ring Oscillator: Freq<br>SRAM: IDDQ SNM WM Iread Iwrite</li>
<li>先进测试工艺<br>Array-based Test Structure, share pads among DUTs</li>
<li>DUT<ul>
<li>Resistance:<br>RS(Resistance Square): to monitor metal&#x2F;dielectric film resistance<br>Film type: Metal(interconection), AA(N+, P+, Salicide), Poly(N, P, Salicide), Well, STI<br>Rm&#x3D;V&#x2F;I&#x3D;(ρ&#x2F;T)<em>(L&#x2F;W), Rs&#x3D;ρ&#x2F;T&#x3D;Rm</em>(L&#x2F;W)  (T:thickness, Rs is for 2-terminal, Rskv is for Kelvin Structure)<br>RC: to monitor the contact&#x2F;via with the under layer</li>
<li>Capacitor:<br>Cap@ 100kHz, 45mV, VH&#x3D;0V, VL&#x3D;0V, Cap&#x3D;Cap*(1e+15)&#x2F;Area<br>Vbd: Search Vh from 0 to 15V @ Ih&#x3D;4e-8A, H&#x3D; sweep V, L&#x3D; GND<br>Ileak: Ilk@ Vh&#x3D;Vdd</li>
<li>Continuity: to verify a continuous path in the circuit, if a variance of Rs in the layer or the pattern is open, it can be detected</li>
<li>Bridge: a bridge means a short between two patterns in the same layer due to an isolation defect(Errors during the mask or etch step)</li>
<li>Diode: DC current-Voltage characterisic</li>
<li>BJT: DC, AC caracteristic</li>
<li>SRAM</li>
<li>CMOS: DC current-Voltage characterisic, VTC 曲线偏移</li>
<li>Ring Oscillator: Frequence, 即时钟频率</li>
<li>MOSFET:<br>DC characteristic: 转移特性，id-Vds<br>Backup Materials: DIBL(Drain insuced barrier lower), Vtsb(Vt @ saturation region wi Body effect), Swing, IDVG, Jginv, GIDL(gate induced drain leak), Idlin, Ioff, Rout, BVDS(击穿电压), Isub(热电子引起基底电流)<br>AC characteristic: cgc, cgb, cgg, 寄生电容等</li>
</ul>
</li>
</ol>
<h5 id="Corner"><a href="#Corner" class="headerlink" title="Corner"></a>Corner</h5><ol>
<li>芯片延迟由 PVT(Process, Voltage, Temperature) 影响</li>
<li>Process corner 工艺角，包括：<ul>
<li>global variation 全局工艺偏差，同一器件在不同芯片上的误差</li>
<li>local variation 局部工艺偏差，同一器件在同一芯片上不同区域的误差</li>
</ul>
</li>
<li>global variation: tt(typical nmos and typical pmos), ff, ss, sf, fs （s 代表 slow，电流小；f 代表 fast）</li>
<li>Corner 仿真实现：STA(Static Timing Analysis)</li>
<li>Monte Carlo 仿真：在容差内随机运算几千次或更多<br><img src="/2023/07/30/%E5%AE%9E%E4%B9%A0/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E4%B8%8Ecorner.jpg" alt="蒙特卡洛与corner"></li>
</ol>
<h5 id="Hspice-仿真"><a href="#Hspice-仿真" class="headerlink" title="Hspice 仿真"></a>Hspice 仿真</h5><ol>
<li>是一个电路仿真eda软件，以网表为输入</li>
<li>网表编写（.sp文件）<ol>
<li>包括元器件描述语句，输入控制语句，输出控制语句等<ul>
<li>.DATA</li>
<li>.DEL LIB</li>
<li>.INCUDE</li>
<li>.IC .NODESET</li>
<li>.LIB</li>
<li>.MODEL</li>
<li>.OP</li>
<li>.OPTION</li>
<li>.PARAM</li>
<li>.TEMP</li>
<li>.TF</li>
<li>.TRAN .DC .AC</li>
<li>.PRINT .PLOT .GRAPH .PROBE .MEASURE</li>
<li>.end</li>
</ul>
</li>
<li>例如 M1 ADDR STG1 GND SBS N1 10u 100u<br>规定了一个名字为M1的MOSFET，漏极、门极、源极、衬底的节点分别命名为ADDR STG1 GND SBS，参考名为N1的Model，width&#x3D;10μ吗，length&#x3D;100μm</li>
<li>.LIB表示库，连接到.lib文件中，网表里的一些元器件需要从里面找对应的model</li>
<li>文件结尾为.end</li>
<li>.TRAN .DC .AC 分别为瞬态，直流，交流分析</li>
<li>.PARAM 为定义各种参数，可理解为define</li>
<li>.OPTION设置条件</li>
<li>等等<br>网表编写可认为是一门语言，需要细细研究</li>
</ol>
</li>
<li><strong>SPICE 测试后得到数据，进行 SPICE 建模，完成后得到器件所有参数，即可打包成一个文档，以库(.lib)和参考模型(.model)的方式提供给所需网表(.sp)供hspice进行仿真</strong></li>
</ol>
<h4 id="Resistance-Extraction"><a href="#Resistance-Extraction" class="headerlink" title="Resistance Extraction"></a>Resistance Extraction</h4><ol>
<li>parameters:<ul>
<li>Rsh: sheet resistance</li>
<li>DW, DL: Delta width and length</li>
<li>TC1R, TC2R: temperature coefficient</li>
<li>RVC0, RVC1, RVC2: Voltage coefficient</li>
<li>JC1A, JC1B, JC2A, JC2B: Current density coefficient</li>
<li>COX, CFOX, CWAPSW, CLOUP: interconnect parastic capacitor</li>
</ul>
</li>
<li>equations:<ul>
<li>R&#x3D;R0<em>k1</em>k2</li>
<li>R0&#x3D;Rsh*(L-2DL)&#x2F;(W-2DW)</li>
<li>k1&#x3D;1.5-1&#x2F;(2+RVC<em>V</em>V), when k1 &gt; 1<br>0.5+1&#x2F;(2+RVC<em>V</em>V), when k1 &lt; 1<br>RVC&#x3D;(RVC0+RVC1<em>W+RVC2</em>L&#x2F;W)&#x2F;(L-2<em>DL)&#x2F;(L-2</em>DL)</li>
<li>K2&#x3D;R0*(TC2R<em>DT</em>DT+TC1R*DT+1)<br>DT&#x3D;T-T(25°C)</li>
</ul>
</li>
<li>extractions:<ul>
<li>examine R @ V&#x3D;0.1V, T&#x3D;25°C, then optimize, get Rsh, DL, DW</li>
<li>examine R @ T&#x3D;25°C, sweep Voltage, get RVC0, RVC1, RVC2</li>
<li>examine R @ V&#x3D;0.1V, sweep Temperature, get TC1R, TC2R</li>
<li>mismatch extraction</li>
<li>corner extraction</li>
<li>parasitics capacitor extraction</li>
</ul>
</li>
<li>model: .lib .sucket</li>
</ol>
<h4 id="Layout-Designs"><a href="#Layout-Designs" class="headerlink" title="Layout Designs"></a>Layout Designs</h4><ol>
<li>版图将电路映射到硅片，是电路功能和性能的物理实现</li>
<li>包括基本元器件版图设计，模块设计-&gt;芯片规划-&gt;布局布线，版图检验分析(DRC(design Rules Check), LVS(Layout Versus Schematic), 电学可靠性检查(ERC, ESD))</li>
<li>绘图层<ul>
<li>N well</li>
<li>Active: 有源层，需要进行场氧(Oxide)</li>
<li>Poly: 多晶硅栅层，互连，生成电阻等</li>
<li>P select, N select: 注入离子，与有源层形成扩散区</li>
<li>Contact: 接触孔层，存在一定阻值，用于连接金属层与目标层</li>
<li>Via: 通孔层</li>
<li>Metal: 金属层，互连</li>
<li>Text: 文字标注层</li>
<li>Pad: 焊盘层，提供芯片内部信号到封装接脚的连接</li>
</ul>
</li>
</ol>
<h4 id="SMEC-BCD-Platform"><a href="#SMEC-BCD-Platform" class="headerlink" title="SMEC BCD Platform"></a>SMEC BCD Platform</h4><ol>
<li>工艺平台，high density, high power, high voltage<br>features: 完整的BCD技术，高附加值器件，高车规模质量，专业的设计支持</li>
<li>IPS: 标准BCD工艺&#x2F;器件 + 嵌入式特色工艺&#x2F;器件(Vertical)<br>180 BCD eflash: 标准BCD工艺&#x2F;器件 + ESFI(3 more masks)<br>BCD-SOI: 使用SOI(Silicon-On-Insulator) Substrate(一种工艺)<br>HVIC: 高压(hig voltage)集成电路</li>
<li>PDK &amp; IP for Industrial and Automotive<ul>
<li>SPICE MODEL: environment temperature range for modeling, Mos Aging Model for Simulation</li>
<li>PDK: DRC &amp; Pcell, EM</li>
<li>IP: Reliability, Functional, Safety</li>
<li>Quality: QMS of FTP R&amp;D organization, Quality Manual of QMS for R&amp;V organization</li>
<li>Readiness: Indurial, Automotive G0&#x2F;G1</li>
</ul>
</li>
<li>180nm BCD 40V SPICE<ul>
<li>SPICE Model: Spectre&#x2F;Hspice&#x2F;ACPS(即所支持的建模eda工具)</li>
<li>PDK: Cadence OA PDK</li>
<li>IP: Standard Cell library, GPIO, Memory Compiler, e-fuse, MTP, OTP, e-flash</li>
</ul>
</li>
<li>document example<ul>
<li>Title: 0.18μm HVIC 120V platform introduction and charater design mannual</li>
<li>Purpose: this design mannual defines…</li>
<li>Scope</li>
<li>Nomenclature</li>
<li>Reference</li>
<li>Responsibility</li>
<li>Subject Content<ul>
<li>Introduction: Document Overview, Process Overciew, Supported potentials, Device Size Setting in Model &amp; PDK, Effect in Spice Model</li>
<li>Device Characterization Summary: Device description, Operation voltage range, Device symbol, layout, Cross section, Spice data vs PCM spec</li>
</ul>
</li>
<li>CMOS, HVMOS(DEMOS, LDMOS, DMOS(VDMOS, DDDMOS)), BJT, Diode, Resistor(DIFF, POLY, NW, Metal), ESD Diode&#x2F;BJT, Capacitor(MIM, MOM, Varactor), IGBT(MOSFET + PNP, VDMOS + PN junction), .etc</li>
</ul>
</li>
</ol>
<h4 id="半导体行业情况"><a href="#半导体行业情况" class="headerlink" title="半导体行业情况"></a>半导体行业情况</h4><h5 id="半导体产业链全景"><a href="#半导体产业链全景" class="headerlink" title="半导体产业链全景"></a>半导体产业链全景</h5><p><img src="/2023/07/30/%E5%AE%9E%E4%B9%A0/%E5%8D%8A%E5%AF%BC%E4%BD%93%E4%BA%A7%E4%B8%9A%E9%93%BE%E5%85%A8%E6%99%AF.png" alt="半导体产业链全景"></p>
<h5 id="半导体产业模式"><a href="#半导体产业模式" class="headerlink" title="半导体产业模式"></a>半导体产业模式</h5><ul>
<li>IDM(Integrated Device Manufacture): Sumsung, Intel</li>
<li>Fabless: 海思, 联发科</li>
<li>Foundry: SMIC, 日月光</li>
</ul>
<h5 id="半导体PRODUCT市值"><a href="#半导体PRODUCT市值" class="headerlink" title="半导体PRODUCT市值"></a>半导体PRODUCT市值</h5><p>半导体产品（$4688亿）包括 集成电路（$3933亿）+ O-S-D（$755亿），其中，集成电路 包括 模拟电路（$588亿）+ 数字电路（$3345亿）</p>
<h5 id="IC-Products"><a href="#IC-Products" class="headerlink" title="IC Products"></a>IC Products</h5><ol>
<li>Memory:<ul>
<li>voliatile(易失性): SRAM(LPSRAM, HS SRAM, eSRAM), DRAM(FP&#x2F;EDO, SDRAM), DDR(Rmbus, eDRAM)</li>
<li>non-voliatile: Flash, EPROM&#x2F;EEROM, MaskROM, MRAM, embedded(Smart Card, National ID)</li>
</ul>
</li>
<li>Logic:<ul>
<li>computing: CPU, MPU, DSP, DPU, Chipset, Graphics, Peripheral, Mem controller</li>
<li>consumer: PDA, GPS, MPEG, DVD&#x2F;VCD, FPGA, Voice synthesis</li>
<li>Automotive: MCU, Sensor</li>
</ul>
</li>
<li>Mixual signal:<ul>
<li>computing: audio processor</li>
<li>communiting: Switching, Sonet, phone, fax, LAN, Router, Bluetooth, Cellular(GPS, CDMA), Modern(xDSL, Cable)</li>
</ul>
</li>
<li>Photo Electronics:<ul>
<li>Image Sensor: CCD, CMOS</li>
<li>Display: LCD driver, LCD controller, μ-Display</li>
<li>Security: Touch chip</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/30/2023%E6%98%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/30/2023%E6%98%A5/" class="post-title-link" itemprop="url">2023春</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-30 20:17:03" itemprop="dateCreated datePublished" datetime="2023-06-30T20:17:03+08:00">2023-06-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-10-05 21:29:13" itemprop="dateModified" datetime="2023-10-05T21:29:13+08:00">2023-10-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E6%9C%9F%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">学期总结</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>树洞上说今天是教务处出分ddl，信号与系统概论的成绩也确实在今天出了，大二生活在这一刻终于画上了句号。这可能是两年来令我最难以忘怀的一个学期，因为它带给我极大的痛苦，但却是最有意义的。</p>
<h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><p><em><strong>彼方尚有荣光在</strong></em></p>
<p>本来以为这学期绩点会大寄的，因为我三个月来根本没有一个时刻有在认真学习，每天都因为某些事emo，但幸好在期末季时三教再现女娲补天，速通四门专业课，最后发现结果好像并没有那么的糟糕。请原谅我不厚道地在这里稍微装一下杯（大佬请跳过 &amp;&amp; 有些得了b+的课没放出来其实）：<strong>（手动tag：引起不适）</strong><br><img src="/2023/06/30/2023%E6%98%A5/1688109417357.png" alt="1688131543947"><br>锐评一下这学期的四门专业课：</p>
<ul>
<li><strong>信号与系统概论</strong>（A-）：本质在学数学，但是老师课上却在讲哲学。。。考前划重点就差把题目放出来了，虽然减轻了复习负担，但是改卷不就更加严格了吗？还有就是题目出得是真的不太严谨，比如填空题第四题明明就没写f(t)是实函数，又怎么能有X(-w)&#x3D;X(w)，R(-w)&#x3D;-R(w)呢？（和助教说了但感觉助教最后还是没给我分😭）</li>
<li><strong>半导体物理</strong>（A）：传说中的翻转教学。本来应该在课堂上获取知识，变成了完完全全靠自己课下自习。或许其本意是帮助我们能够更加深刻的理解知识，毕竟自己学会跟能够把知识讲出来是不一样的，但是这门又难又基础的课，自学起来真的非常非常吃力。期末考的卷子也不好评价，考的感觉好多都没学过，做题时一堆不确定的，不过老师和助教都很好，浅浅捞了一把。</li>
<li><strong>计算机软件基础</strong>（A）：我觉得课的内容真心挺好的，学会了基本的Linux与数据结构，就是卷子出的太简单了（期末和期中都是半个小时就做完了），然后给分强行正态。。。但是获得了赵大爷的握手😂</li>
<li><strong>嵌入式处理器与芯片系统设计</strong>（A）：处理器的前世今生，基本的流水线设计，分支预测，页表和缓存，I&#x2F;O管理，等等，其实课上和ppt都讲得十分清楚，理论课10分能打9分，确实学到了很多东西；实验课的话说实话不太行，特别是每次做实验之前都没有讲清楚实验原理，然后就有一堆的bug，并且很多实验都是机械的重复同一个操作（十分无语且崩溃）；另外期末考真的是太tm难了！！！</li>
</ul>
<p>学微电子真的是啥都要学，确实会让人感慨“微电子🐶不学”。但实际上，我觉得其实很多课都能够考前速通，除了数学。只有数学好的人才是真的强，真的很佩服数学好的人啊！（比如棍、隽立和俊宇）</p>
<h2 id="感情"><a href="#感情" class="headerlink" title="感情"></a>感情</h2><p><em><strong>我堕入情网你却在网外看</strong></em></p>
<p>这学期喜欢上了一个不喜欢我的女生，这也是我这学期痛苦的来源。</p>
<p>我们是在一次我认为很巧合的情况下认识的，并且相处得十分愉快。佛说，五百次回眸，才换来今生一次擦肩而过。我觉得我们的相遇真的十分的有缘，并且内心在告诉我，树安你就是喜欢她。于是之后我经常主动找她聊天。她的回复也挺热情的，看上去也有聊天的欲望。<br>后来她主动约我去看十大复赛的音乐节，这是我们第二次见面。由于音乐节有点无聊，于是我们在南区里边走边聊。可能因为是e人，她话说得比较多，我反而表现得比较收敛。她非常爱笑，笑起来真的非常的好看，给人带来快乐。之后接下来几天的线上聊天，我们聊得都还算比较好。<br>我想继续约她出来玩，但人家好像真的很忙，我约了几次都没有成功。在这之后，聊天都是我在主动，她的回复感觉远没有之前热情了。这时按道理应该及时止损，但当时恋爱脑的我，一边处于“她是不是其实不喜欢我”的深深内耗中，一边却依旧抱有一丝的希望。第三次见面，也是最后一次见面，是我约的她出去吃饭。虽然聊了两小时没停下来，但我知道，我们之间已经不可能了。回去之后，由于一些原因，我们就闹僵了。</p>
<p>上面的故事或许说得有点轻描淡写，但实际上三个月来，可以用crazy来形容我，恋爱脑的内耗或许只有亲身体会才知道有多么痛苦，每天都在猜测对方的心意，分析她回复的每一句话的情感。现在回过头来看，或许别人从一开始仅仅就只是想交个朋友而已，只是我自己比较上头，属于是有点小丑了。</p>
<p><strong>但是爱而不得那不是人生常事吗？</strong></p>
<h2 id="有意思的东西"><a href="#有意思的东西" class="headerlink" title="有意思的东西"></a>有意思的东西</h2><p><em><strong>抬头就有一片星光</strong></em></p>
<ul>
<li>狠狠体会了一把在图书馆被恰v是什么感觉😎<br><img src="/2023/06/30/2023%E6%98%A5/1688108763378.png" alt="1688108763378"></li>
<li>狠狠体会了一把以一种奇怪的方式被挂树洞😰（真的有难酮！）<br><img src="/2023/06/30/2023%E6%98%A5/image.png" alt="Alt text"></li>
</ul>
<h2 id="友情与致谢"><a href="#友情与致谢" class="headerlink" title="友情与致谢"></a>友情与致谢</h2><p><em><strong>人生如路，朋友如雾。难得知心，几经风暴，为着我不退半步，正是你。</strong></em></p>
<p>我不敢想象，如果这学期没有了好朋友的支持与鼓励，该变得多么的糟糕！</p>
<ul>
<li>First of all，真的非常非常非常地感谢隽立，在我每次（其实是每天）都快要崩溃的时候，拯救了我；每当遇到想不懂的事，想不通的问题，都能给我码顺；也是隽立给了我期末季补天的无限动力——人生在世，又能有多少知己？所以能有清华✌做好兄弟，真的是上辈子修来的福气。<br><img src="/2023/06/30/2023%E6%98%A5/1688108889614.png" alt="1688108889614"></li>
<li>还有天宝，在我心情不好的时候陪我唱k、喝酒(虽然我喝不了一点)，让我感受到，我在复旦其实不是一个人；天宝唱功也越来越强了（都快赶上我了😋），我愿称之为我旦华晨宇。数院大神俊宇，以及杰哥和棍他们，每天都在提醒我要学习，但是现在我的数学跟他们差距真的很大，暑假真的要好好沉淀。</li>
<li>当然少不了大一的3位舍友，这里特别点名准“莙政学者”zh到了<strong>阿美莉卡</strong>之后“苟富贵，毋相忘”。还有微电的朋友们也给了我很多学习和生活上的帮助，他们每个人都真的是又强又有趣，真的很幸运能有如此优秀的同学。</li>
<li>还有很多很多的friends……</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><em><strong>明天你好，我只能奔跑</strong></em></p>
<p>说实话，大学两年了，我发现自己好像真的没做出什么成绩来。特别是这学期挫败感尤为地强烈：集创赛没进分赛区决赛，喜欢的女生不喜欢我，大二都结束了才刚刚开始接触科研……绩点似乎能够聊以自慰，但也有好多好多人绩点比我高，并且我感觉拿A也只是意味着考试考得好罢了，代表不了什么。所以我常常自我怀疑，是不是自己真的没有什么独特之处？自己真的适合学微电子吗？</p>
<p>可能确实是这样的，我也确确实实只是个极其普通的人罢了。我现在只希望，能做自己喜欢做的事，能够实现一些小小的愿望，然后好好过完这一生。</p>
<p>明天你好，七月你好。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/28/Life/My-New-Post/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/28/Life/My-New-Post/" class="post-title-link" itemprop="url">My New Post</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-28 13:24:39" itemprop="dateCreated datePublished" datetime="2023-06-28T13:24:39+08:00">2023-06-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-06-30 11:00:07" itemprop="dateModified" datetime="2023-06-30T11:00:07+08:00">2023-06-30</time>
              </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这是我的第一篇blog</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/28/Life/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="Treeby">
      <meta itemprop="description" content="La vie est belle">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Amusement Park">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/28/Life/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-28 12:28:07" itemprop="dateCreated datePublished" datetime="2023-06-28T12:28:07+08:00">2023-06-28</time>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Treeby"
      src="/images/author.jpg">
  <p class="site-author-name" itemprop="name">Treeby</p>
  <div class="site-description" itemprop="description">La vie est belle</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/satreeby" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;satreeby" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:satreeby@gmail.com" title="E-Mail → mailto:satreeby@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      friendly link
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://ltnsxd.github.io/" title="https:&#x2F;&#x2F;ltnsxd.github.io" rel="noopener" target="_blank">LTNS</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Treeby</span>
</div><div>
<!--添加网站运行时间-->
<span>小破站已经在风雨中度过了</span>
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    function createtime() {
        const startTime = '06/28/2023 20:13:14',
            start = new Date(startTime)
        let mill = new Date() - start,
            seconds = Math.floor(mill / 1000),
            mins = Math.floor(seconds / 60),
            hours = Math.floor(mins / 60),
            days = Math.floor(hours / 24)
        const time = {
            seconds: seconds - mins * 60,
            mins: mins - hours * 60,
            hours: hours - days * 24,
        }
        for (const k in time) {
            time[k] = `${time[k]}`.padStart(2, '0')
        }
        document.getElementById("timeDate").innerHTML = ` ${days} 天`
        document.getElementById("times").innerHTML = ` ${time.hours} 小时 ${time.mins} 分 ${time.seconds} 秒`
    }
    setInterval(createtime, 500)
</script>
</div>
<script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://example.com/default-index/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '6zRY3CX7EmWrElP42NGxNKsd-gzGzoHsz',
      appKey     : '1v8aaMV8PG0GgDS8Sqwnt9EM',
      placeholder: "帅呆啦！",
      avatar     : 'monsterid',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
